<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kubernetes Observability & Monitoring: Complete Deep Technical Guide - Alex Susanu</title>
    <link rel="stylesheet" href="../assets/css/main.css">
    <style>
        /* Note-specific styles that extend the main CSS */
        .note-page {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .note-container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            min-height: 100vh;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
        }
        
        .note-header {
            background: linear-gradient(135deg, #4a90e2, #357abd);
            color: white;
            padding: 30px;
            text-align: center;
        }
        
        .back-nav {
            background: #f8f9ff;
            padding: 15px 30px;
            border-bottom: 2px solid #e8f0ff;
        }
        
        .back-btn {
            background: #4a90e2;
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            border-radius: 5px;
            font-size: 14px;
            transition: all 0.3s ease;
        }
        
        .back-btn:hover {
            background: #357abd;
        }
        
        .note-content-wrapper {
            padding: 40px 30px;
        }
        
        .note-meta {
            color: #666;
            font-style: italic;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 1px solid #e8f0ff;
        }
        
        .note-footer {
            background: #f8f9ff;
            padding: 20px 30px;
            text-align: center;
            color: #666;
            border-top: 2px solid #e8f0ff;
        }
    </style>
</head>
<body class="note-page">
    <div class="note-container">
        <div class="note-header">
            <h1>Kubernetes Observability & Monitoring: Complete Deep Technical Guide</h1>
        </div>
        
        <div class="back-nav">
            <a href="../index.html" class="back-btn">← Back to Knowledge Base</a>
        </div>
        
        <div class="note-content-wrapper">
            <div class="note-meta">
                DevOps (k8s) • Updated May 31, 2025
            </div>
            
            <div class="note-tags">
                <span class="tag">kubernetes</span><span class="tag">observability</span><span class="tag">monitoring</span><span class="tag">logging</span><span class="tag">metrics</span><span class="tag">tracing</span><span class="tag">prometheus</span><span class="tag">grafana</span><span class="tag">jaeger</span>
            </div>
            
            <div class="note-content">
                <h2>Kubernetes Observability & Monitoring: Complete Deep Technical Guide</h2>
<h3>Introduction to Kubernetes Observability</h3>
<strong>Observability</strong> is the ability to understand what's happening inside your Kubernetes cluster and applications by examining their external outputs. It's built on three pillars: <strong>logs</strong>, <strong>metrics</strong>, and <strong>traces</strong>.
<h4>The Three Pillars of Observability</h4>
<strong>Logs</strong> - Detailed records of what happened
<ul><li>Application logs, error messages, audit trails</li>
<li>Useful for debugging specific issues and understanding application behavior</li>
<strong>Metrics</strong> - Numerical measurements over time  
<li>CPU usage, memory consumption, request rates, error rates</li>
<li>Useful for alerting, capacity planning, and performance monitoring</li>
<strong>Traces</strong> - Request flows through distributed systems
<li>How a single request travels through multiple services</li>
<li>Useful for understanding performance bottlenecks and service dependencies</li>
<h4>Why Kubernetes Observability is Complex</h4>
<strong>Distributed by Nature:</strong>
<pre><code>Single Request → API Gateway → Auth Service → Business Logic → Database
                     ↓              ↓              ↓              ↓
                 Container 1    Container 2    Container 3    External
                   Node A         Node B         Node C       Service</code></pre>
<strong>Dynamic Environment:</strong>
<li>Pods are created and destroyed constantly</li>
<li>Services can scale up and down automatically</li>
<li>Workloads move between nodes</li>
<li>Network topology changes frequently</li>
<strong>Multiple Layers:</strong>
<li><strong>Infrastructure</strong> - Nodes, network, storage</li>
<li><strong>Platform</strong> - Kubernetes components (API server, kubelet, etcd)</li>
<li><strong>Application</strong> - Your microservices and workloads</li>
<li><strong>User Experience</strong> - End-to-end request performance</li>
<h3>Kubernetes Logging Architecture Deep Dive</h3>
<h4>How Kubernetes Logging Works</h4>
<strong>Container-Level Logging:</strong>
Every container's stdout and stderr streams are automatically captured by the container runtime and stored as log files on the node.
<strong>Log Storage Locations:</strong>
<pre><code><h2>Container logs on node</h2>
/var/log/pods/<namespace>_<pod-name>_<pod-uid>/<container-name>/
├── 0.log     # Current log file
├── 0.log.1   # Rotated log file
└── 0.log.2   # Older rotated log file
<h2>Symbolic links for easy access</h2>
/var/log/containers/
└── <pod-name>_<namespace>_<container-name>-<container-id>.log -> /var/log/pods/...</code></pre>
<strong>Log Rotation:</strong>
Container runtime automatically rotates logs to prevent disk space issues:
<li>Default: 10MB per file, keep 5 files</li>
<li>Total: ~50MB per container maximum</li>
<h4>Application Logging Patterns</h4>
<p>#### Structured Logging (JSON)
<pre><code>// Go application with structured logging
package main</p>
<p>import (
    "log/slog"
    "os"
)</p>
<p>func main() {
    // Create structured logger
    logger := slog.New(slog.NewJSONHandler(os.Stdout, nil))
    
    // Log with structured data
    logger.Info("Application starting",
        "version", "1.2.3",
        "port", 8080,
        "environment", "production")
    
    logger.Error("Database connection failed",
        "error", "connection timeout",
        "host", "postgres.database.svc.cluster.local",
        "port", 5432,
        "retry_count", 3)
}</code></pre></p>
<strong>Output:</strong>
<pre><code>{"time":"2024-01-15T10:30:00Z","level":"INFO","msg":"Application starting","version":"1.2.3","port":8080,"environment":"production"}
{"time":"2024-01-15T10:30:05Z","level":"ERROR","msg":"Database connection failed","error":"connection timeout","host":"postgres.database.svc.cluster.local","port":5432,"retry_count":3}</code></pre>
<p>#### Multi-Line Log Handling
<pre><code><h2>Java application with stack traces</h2>
apiVersion: v1
kind: Pod
metadata:
  name: java-app
  annotations:
    # Tell log collector to handle multi-line logs
    fluentbit.io/parser: java-multiline
spec:
  containers:
  - name: app
    image: java-app:latest
    # Java app logs stack traces across multiple lines</code></pre></p>
<h4>Log Collection Strategies</h4>
<p>#### Node-Level Log Collection with DaemonSet</p>
<strong>Fluent Bit DaemonSet:</strong>
<pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      serviceAccount: fluent-bit
      tolerations:
      - operator: Exists  # Run on all nodes including masters
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.0.8
        ports:
        - containerPort: 2020
          name: metrics
        volumeMounts:
        # Access to container logs
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        # Configuration
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config</code></pre>
<strong>Fluent Bit Configuration:</strong>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: kube-system
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020
<p>[INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            cri
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On</p>
<p>[FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Merge_Log           On
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off</p>
<p>[FILTER]
        Name                nest
        Match               kube.*
        Operation           lift
        Nested_under        kubernetes
        Add_prefix          kubernetes_</p>
<p>[OUTPUT]
        Name                es
        Match               *
        Host                elasticsearch.logging.svc.cluster.local
        Port                9200
        Logstash_Format     On
        Logstash_Prefix     kubernetes
        Time_Key            @timestamp
        Time_Key_Format     %Y-%m-%dT%H:%M:%S.%L
        Include_Tag_Key     true
        Tag_Key             tag</p>
<p>parsers.conf: |
    [PARSER]
        Name        cri
        Format      regex
        Regex       ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]<em>) (?<message>.</em>)$
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z</p>
<p>[PARSER]
        Name        json
        Format      json
        Time_Key    timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L</p>
<p>[PARSER]
        Name        java-multiline
        Format      regex
        Regex       /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3})\s+(?<level>[^\s]+).<em>(?<message>.</em>)/
        Time_Key    time
        Time_Format %Y-%m-%d %H:%M:%S.%L</code></pre></p>
<p>#### Sidecar Log Collection Pattern</p>
<strong>Application with Log Sidecar:</strong>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: app-with-log-sidecar
spec:
  containers:
  # Main application
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
    # App writes logs to files in /var/log/app
  
  # Log collection sidecar
  - name: log-collector
    image: fluent/fluent-bit:latest
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
      readOnly: true
    - name: sidecar-config
      mountPath: /fluent-bit/etc
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
  
  volumes:
  - name: app-logs
    emptyDir: {}
  - name: sidecar-config
    configMap:
      name: sidecar-fluent-bit-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: sidecar-fluent-bit-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
<p>[INPUT]
        Name              tail
        Path              /var/log/app/*.log
        Parser            json
        Tag               app.${POD_NAME}
        Refresh_Interval  5</p>
<p>[FILTER]
        Name    modify
        Match   *
        Add     pod_name ${POD_NAME}
        Add     namespace ${NAMESPACE}</p>
<p>[OUTPUT]
        Name    forward
        Match   *
        Host    log-aggregator.logging.svc.cluster.local
        Port    24224</code></pre></p>
<h4>Centralized Logging with EFK Stack</h4>
<p>#### Elasticsearch Cluster
<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
      storageClassName: fast-ssd
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          sysctl -w vm.max_map_count=262144
          echo 'vm.max_map_count=262144' >> /etc/sysctl.conf
        securityContext:
          privileged: true
      
      containers:
      - name: elasticsearch
        image: elasticsearch:8.5.0
        env:
        - name: cluster.name
          value: "kubernetes-logs"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: xpack.security.enabled
          value: "false"
        
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        
        resources:
          requests:
            memory: 4Gi
            cpu: 1000m
          limits:
            memory: 4Gi
            cpu: 2000m</code></pre></p>
<p>#### Kibana Dashboard
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: kibana:8.5.0
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: SERVER_NAME
          value: "kibana"
        - name: SERVER_BASEPATH
          value: ""
        
        ports:
        - containerPort: 5601
          name: http
        
        resources:
          requests:
            memory: 1Gi
            cpu: 500m
          limits:
            memory: 2Gi
            cpu: 1000m
        
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 30
          periodSeconds: 10
        
        livenessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
          periodSeconds: 30</code></pre></p>
<h4>Log Analysis and Alerting</h4>
<p>#### Log-Based Alerting with ElastAlert
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: elastalert-config
  namespace: logging
data:
  config.yaml: |
    rules_folder: /opt/elastalert/rules
    run_every:
      minutes: 1
    buffer_time:
      minutes: 15
    es_host: elasticsearch.logging.svc.cluster.local
    es_port: 9200
    writeback_index: elastalert_status
    alert_time_limit:
      days: 2</p>
<p>error_rate_rule.yaml: |
    name: High Error Rate
    type: frequency
    index: kubernetes-*
    num_events: 50
    timeframe:
      minutes: 5
    filter:
    - term:
        level: "ERROR"
    alert:
    - "slack"
    slack:
      slack_webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
      slack_channel_override: "#alerts"
      slack_title: "High Error Rate Detected"
      slack_title_link: "http://kibana.example.com"</p>
<p>application_down_rule.yaml: |
    name: Application Down
    type: flatline
    index: kubernetes-*
    threshold: 0
    timeframe:
      minutes: 10
    filter:
    - term:
        kubernetes_labels_app: "critical-app"
    alert:
    - "email"
    email:
    - "oncall@company.com"
    smtp_host: "smtp.company.com"
    smtp_port: 587
    from_addr: "alerts@company.com"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elastalert
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elastalert
  template:
    metadata:
      labels:
        app: elastalert
    spec:
      containers:
      - name: elastalert
        image: jertel/elastalert2:latest
        volumeMounts:
        - name: elastalert-config
          mountPath: /opt/elastalert/config.yaml
          subPath: config.yaml
        - name: elastalert-config
          mountPath: /opt/elastalert/rules/error_rate_rule.yaml
          subPath: error_rate_rule.yaml
        - name: elastalert-config
          mountPath: /opt/elastalert/rules/application_down_rule.yaml
          subPath: application_down_rule.yaml
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
      volumes:
      - name: elastalert-config
        configMap:
          name: elastalert-config</code></pre></p>
<h3>Metrics Collection Deep Dive</h3>
<h4>Prometheus Architecture</h4>
<strong>How Prometheus Works:</strong>
1. <strong>Scraping</strong> - Prometheus pulls metrics from targets at regular intervals
2. <strong>Storage</strong> - Time-series data stored in local TSDB (Time Series Database)
3. <strong>Querying</strong> - PromQL (Prometheus Query Language) for analysis
4. <strong>Alerting</strong> - Rules evaluate metrics and trigger alerts
<strong>Prometheus Components:</strong>
<li><strong>Prometheus Server</strong> - Core server that scrapes and stores metrics</li>
<li><strong>Pushgateway</strong> - For batch jobs that can't be scraped</li>
<li><strong>Alertmanager</strong> - Handles alerts from Prometheus server</li>
<li><strong>Exporters</strong> - Applications that expose metrics for Prometheus</li>
<h4>Kubernetes Metrics Sources</h4>
<p>#### Node-Level Metrics (Node Exporter)
<pre><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
        prometheus.io/path: "/metrics"
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.5.0
        args:
        - '--path.rootfs=/host'
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
        - '--collector.systemd'
        - '--collector.processes'
        
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: metrics
        
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host
          readOnly: true
        
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 100m
      
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
      
      tolerations:
      - operator: Exists
        effect: NoSchedule</code></pre></p>
<p>#### Kubernetes Component Metrics (kube-state-metrics)
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.7.0
        args:
        - --port=8080
        - --resources=pods,deployments,services,nodes,configmaps,secrets,persistentvolumes,persistentvolumeclaims,namespaces,endpoints,statefulsets,daemonsets,replicasets,jobs,cronjobs
        - --metric-labels-allowlist=pods=[<em>],deployments=[</em>],services=[*]
        
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 8081
          name: telemetry
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
        
        resources:
          requests:
            memory: 128Mi
            cpu: 100m
          limits:
            memory: 256Mi
            cpu: 200m</code></pre></p>
<p>#### Application Metrics Integration</p>
<strong>Go Application with Prometheus Metrics:</strong>
<pre><code>package main
<p>import (
    "net/http"
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)</p>
<p>var (
    // Counter - monotonically increasing value
    httpRequestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status_code"},
    )
    
    // Histogram - distribution of values
    httpRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "Duration of HTTP requests in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
    
    // Gauge - value that can go up and down
    activeConnections = prometheus.NewGauge(
        prometheus.GaugeOpts{
            Name: "active_connections",
            Help: "Number of active connections",
        },
    )
    
    // Summary - like histogram but with quantiles
    responseSize = prometheus.NewSummaryVec(
        prometheus.SummaryOpts{
            Name: "response_size_bytes",
            Help: "Response size in bytes",
            Objectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},
        },
        []string{"endpoint"},
    )
)</p>
<p>func init() {
    // Register metrics with Prometheus
    prometheus.MustRegister(httpRequestsTotal)
    prometheus.MustRegister(httpRequestDuration)
    prometheus.MustRegister(activeConnections)
    prometheus.MustRegister(responseSize)
}</p>
<p>func metricsMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        
        // Track active connections
        activeConnections.Inc()
        defer activeConnections.Dec()
        
        // Wrap response writer to capture status code and size
        wrapped := &responseWriter{ResponseWriter: w, statusCode: 200}
        
        next.ServeHTTP(wrapped, r)
        
        // Record metrics
        duration := time.Since(start).Seconds()
        httpRequestDuration.WithLabelValues(r.Method, r.URL.Path).Observe(duration)
        httpRequestsTotal.WithLabelValues(r.Method, r.URL.Path, fmt.Sprintf("%d", wrapped.statusCode)).Inc()
        responseSize.WithLabelValues(r.URL.Path).Observe(float64(wrapped.size))
    })
}</p>
<p>func main() {
    // Expose metrics endpoint
    http.Handle("/metrics", promhttp.Handler())
    
    // Application endpoints
    mux := http.NewServeMux()
    mux.HandleFunc("/api/users", handleUsers)
    mux.HandleFunc("/api/health", handleHealth)
    
    // Wrap with metrics middleware
    http.Handle("/", metricsMiddleware(mux))
    
    log.Fatal(http.ListenAndServe(":8080", nil))
}</code></pre></p>
<strong>Kubernetes Deployment with Metrics:</strong>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: metrics-app
  template:
    metadata:
      labels:
        app: metrics-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: metrics-app:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: METRICS_PORT
          value: "9090"
        resources:
          requests:
            memory: 256Mi
            cpu: 200m
          limits:
            memory: 512Mi
            cpu: 500m</code></pre>
<h4>Prometheus Configuration</h4>
<p>#### Complete Prometheus Setup
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'kubernetes-cluster'
        region: 'us-west-2'</p>
<p>rule_files:
    - "/etc/prometheus/rules/*.yml"</p>
<p>alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093</p>
<p>scrape_configs:
    # Prometheus itself
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']</p>
<p># Kubernetes API server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https</p>
<p># Kubernetes nodes
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics</p>
<p># Node Exporter
    - job_name: 'node-exporter'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name]
        action: keep
        regex: node-exporter
      - source_labels: [__meta_kubernetes_endpoint_port_name]
        action: keep
        regex: metrics</p>
<p># kube-state-metrics
    - job_name: 'kube-state-metrics'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name]
        action: keep
        regex: kube-state-metrics</p>
<p># Application pods with prometheus annotations
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name</p>
<p>alerts.yml: |
    groups:
    - name: kubernetes-apps
      rules:
      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"</p>
<p>- alert: KubernetesPodNotReady
        expr: kube_pod_status_phase{phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes Pod not ready (instance {{ $labels.instance }})"
          description: "Pod {{ $labels.pod }} has been in a non-ready state for longer than 5 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"</p>
<p>- name: kubernetes-resources
      rules:
      - alert: KubernetesNodeOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Node out of disk (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"</p>
<p>- alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.40.0
        args:
        - '--storage.tsdb.retention.time=30d'
        - '--storage.tsdb.path=/prometheus'
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--web.enable-lifecycle'
        - '--web.enable-admin-api'
        
        ports:
        - containerPort: 9090
          name: http
        
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-storage
          mountPath: /prometheus
        
        resources:
          requests:
            memory: 2Gi
            cpu: 1000m
          limits:
            memory: 4Gi
            cpu: 2000m
      
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-storage
        persistentVolumeClaim:
          claimName: prometheus-storage</code></pre></p>
<h4>Grafana Dashboards</h4>
<p>#### Grafana Deployment
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:9.3.0
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-credentials
              key: admin-password
        - name: GF_INSTALL_PLUGINS
          value: "grafana-kubernetes-app,grafana-piechart-panel"
        - name: GF_SERVER_ROOT_URL
          value: "https://grafana.example.com"
        
        ports:
        - containerPort: 3000
          name: http
        
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-provisioning
          mountPath: /etc/grafana/provisioning
        
        resources:
          requests:
            memory: 512Mi
            cpu: 200m
          limits:
            memory: 1Gi
            cpu: 500m
        
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 10
        
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 30
      
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-storage
      - name: grafana-provisioning
        configMap:
          name: grafana-provisioning</code></pre></p>
<p>#### Grafana Provisioning Configuration
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-provisioning
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      isDefault: true
      editable: true</p>
<p>dashboards.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      updateIntervalSeconds: 10
      allowUiUpdates: true
      options:
        path: /var/lib/grafana/dashboards</p>
<p>kubernetes-cluster-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Kubernetes Cluster Monitoring",
        "description": "Comprehensive Kubernetes cluster monitoring dashboard",
        "panels": [
          {
            "title": "Cluster CPU Usage",
            "type": "stat",
            "targets": [
              {
                "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                "legendFormat": "CPU Usage %"
              }
            ]
          },
          {
            "title": "Cluster Memory Usage",
            "type": "stat",
            "targets": [
              {
                "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
                "legendFormat": "Memory Usage %"
              }
            ]
          },
          {
            "title": "Pod Status",
            "type": "piechart",
            "targets": [
              {
                "expr": "kube_pod_status_phase",
                "legendFormat": "{{ phase }}"
              }
            ]
          },
          {
            "title": "HTTP Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total[5m])) by (service)",
                "legendFormat": "{{ service }}"
              }
            ]
          }
        ]
      }
    }</code></pre></p>
<h3>Health Checks & Probes Deep Dive</h3>
<h4>Understanding Kubernetes Probes</h4>
<p>Kubernetes provides three types of health checks to monitor container and application health:</p>
<p>#### Liveness Probes
<strong>Purpose:</strong> Detect when container is stuck and needs restart
<pre><code>livenessProbe:
  httpGet:
    path: /health
    port: 8080
    httpHeaders:
    - name: Custom-Header
      value: liveness-check
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1</code></pre></p>
<strong>What happens when liveness probe fails:</strong>
1. <strong>Failure detected</strong> - Probe fails <code>failureThreshold</code> times
2. <strong>Container killed</strong> - kubelet kills the container
3. <strong>Restart policy applied</strong> - Container restarted based on restart policy
4. <strong>Pod events logged</strong> - Failure recorded in pod events
<p>#### Readiness Probes
<strong>Purpose:</strong> Detect when container is ready to receive traffic
<pre><code>readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 2
  successThreshold: 1</code></pre></p>
<strong>What happens when readiness probe fails:</strong>
1. <strong>Pod marked not ready</strong> - Pod status shows not ready
2. <strong>Removed from service</strong> - Pod IP removed from service endpoints
3. <strong>No traffic received</strong> - Load balancer stops sending traffic
4. <strong>Container not restarted</strong> - Container continues running
<p>#### Startup Probes
<strong>Purpose:</strong> Handle slow-starting containers
<pre><code>startupProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 30  # Allow 150 seconds for startup</code></pre></p>
<strong>Startup probe behavior:</strong>
<li><strong>Runs first</strong> - Before liveness and readiness probes</li>
<li><strong>One-time check</strong> - Stops running after first success</li>
<li><strong>Protects slow starts</strong> - Prevents liveness probe from killing slow-starting containers</li>
<h4>Probe Implementation Patterns</h4>
<p>#### HTTP Health Endpoints
<pre><code>// Go application health endpoint
package main</p>
<p>import (
    "encoding/json"
    "net/http"
    "time"
)</p>
<p>type HealthStatus struct {
    Status      string            <code>json:"status"</code>
    Timestamp   time.Time         <code>json:"timestamp"</code>
    Checks      map[string]string <code>json:"checks"</code>
    Version     string            <code>json:"version"</code>
    Uptime      string            <code>json:"uptime"</code>
}</p>
<p>var startTime = time.Now()</p>
<p>func healthHandler(w http.ResponseWriter, r *http.Request) {
    health := HealthStatus{
        Status:    "ok",
        Timestamp: time.Now(),
        Checks:    make(map[string]string),
        Version:   "1.2.3",
        Uptime:    time.Since(startTime).String(),
    }
    
    // Check database connection
    if err := checkDatabase(); err != nil {
        health.Status = "unhealthy"
        health.Checks["database"] = "failed: " + err.Error()
        w.WriteHeader(http.StatusServiceUnavailable)
    } else {
        health.Checks["database"] = "ok"
    }
    
    // Check external dependencies
    if err := checkExternalAPI(); err != nil {
        health.Checks["external_api"] = "failed: " + err.Error()
        // Don't mark as unhealthy for external dependencies
    } else {
        health.Checks["external_api"] = "ok"
    }
    
    // Check internal components
    health.Checks["memory"] = checkMemoryUsage()
    health.Checks["disk"] = checkDiskSpace()
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(health)
}</p>
<p>func readinessHandler(w http.ResponseWriter, r *http.Request) {
    // Readiness check - only essential dependencies
    if err := checkDatabase(); err != nil {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Database not ready"))
        return
    }
    
    if !isApplicationReady() {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Application not ready"))
        return
    }
    
    w.WriteHeader(http.StatusOK)
    w.Write([]byte("Ready"))
}</p>
<p>func main() {
    http.HandleFunc("/health", healthHandler)
    http.HandleFunc("/ready", readinessHandler)
    http.HandleFunc("/", applicationHandler)
    
    http.ListenAndServe(":8080", nil)
}</code></pre></p>
<p>#### TCP and Command Probes
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: probe-examples
spec:
  containers:
  - name: app
    image: myapp:latest
    
    # HTTP probe (most common)
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 10
    
    # TCP probe (for non-HTTP services)
    readinessProbe:
      tcpSocket:
        port: 5432
      initialDelaySeconds: 5
      periodSeconds: 5
    
    # Command probe (custom check)
    startupProbe:
      exec:
        command:
        - /bin/sh
        - -c
        - "test -f /tmp/ready && curl -f http://localhost:8080/health"
      initialDelaySeconds: 10
      periodSeconds: 5
      failureThreshold: 30</code></pre></p>
<h4>Advanced Health Check Patterns</h4>
<p>#### Multi-Service Health Aggregation
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: microservice-with-dependencies
spec:
  containers:
  - name: main-service
    image: main-service:latest
    env:
    - name: HEALTH_CHECK_DEPENDENCIES
      value: "database,cache,messaging"
    - name: DATABASE_URL
      value: "postgres://user:pass@db:5432/myapp"
    - name: REDIS_URL
      value: "redis://cache:6379"
    - name: MESSAGING_URL
      value: "amqp://guest:guest@rabbitmq:5672/"
    
    livenessProbe:
      httpGet:
        path: /health/liveness
        port: 8080
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 3
    
    readinessProbe:
      httpGet:
        path: /health/readiness
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 5
      failureThreshold: 2
    
    startupProbe:
      httpGet:
        path: /health/startup
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 12  # 2 minutes for startup</code></pre></p>
<p>#### Graceful Shutdown Integration
<pre><code>// Go application with graceful shutdown
package main</p>
<p>import (
    "context"
    "net/http"
    "os"
    "os/signal"
    "sync/atomic"
    "syscall"
    "time"
)</p>
<p>var (
    healthy int32 = 1
    ready   int32 = 1
)</p>
<p>func healthHandler(w http.ResponseWriter, r *http.Request) {
    if atomic.LoadInt32(&healthy) == 1 {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("Healthy"))
    } else {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Unhealthy"))
    }
}</p>
<p>func readinessHandler(w http.ResponseWriter, r *http.Request) {
    if atomic.LoadInt32(&ready) == 1 {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("Ready"))
    } else {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Not Ready"))
    }
}</p>
<p>func main() {
    // Setup HTTP server
    mux := http.NewServeMux()
    mux.HandleFunc("/health", healthHandler)
    mux.HandleFunc("/ready", readinessHandler)
    mux.HandleFunc("/", applicationHandler)
    
    server := &http.Server{
        Addr:    ":8080",
        Handler: mux,
    }
    
    // Start server
    go func() {
        if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            log.Fatal("Server failed:", err)
        }
    }()
    
    // Wait for shutdown signal
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    <-quit
    
    log.Println("Shutting down server...")
    
    // Mark as not ready (stop receiving new traffic)
    atomic.StoreInt32(&ready, 0)
    
    // Wait for existing connections to drain
    time.Sleep(10 * time.Second)
    
    // Mark as unhealthy
    atomic.StoreInt32(&healthy, 0)
    
    // Graceful shutdown with timeout
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    if err := server.Shutdown(ctx); err != nil {
        log.Fatal("Server forced to shutdown:", err)
    }
    
    log.Println("Server exiting")
}</code></pre></p>
<h3>Distributed Tracing Deep Dive</h3>
<h4>What is Distributed Tracing?</h4>
<strong>Distributed tracing</strong> tracks requests as they flow through multiple services in a microservices architecture. Each request gets a unique <strong>trace ID</strong>, and each service operation gets a <strong>span ID</strong>.
<strong>Trace Structure:</strong>
<pre><code>Trace ID: abc123 (entire request journey)
├── Span: API Gateway (50ms)
│   └── Span: Auth Service (20ms)
│       └── Span: Database Query (15ms)
├── Span: Business Logic Service (100ms)
│   ├── Span: Cache Lookup (5ms)
│   └── Span: Database Query (30ms)
└── Span: Notification Service (25ms)
    └── Span: External API Call (20ms)</code></pre>
<h4>OpenTelemetry Integration</h4>
<p>#### OpenTelemetry Collector
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      zipkin:
        endpoint: 0.0.0.0:9411</p>
<p>processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        limit_mib: 512</p>
<p>exporters:
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
      zipkin:
        endpoint: http://zipkin:9411/api/v2/spans
      prometheus:
        endpoint: "0.0.0.0:8889"</p>
<p>service:
      pipelines:
        traces:
          receivers: [otlp, jaeger, zipkin]
          processors: [memory_limiter, batch]
          exporters: [jaeger]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector:latest
        command: ["otelcol", "--config=/etc/config/config.yaml"]
        volumeMounts:
        - name: config
          mountPath: /etc/config
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 14250
          name: jaeger-grpc
        - containerPort: 14268
          name: jaeger-http
        - containerPort: 9411
          name: zipkin
        - containerPort: 8889
          name: metrics
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 512Mi
            cpu: 200m
      volumes:
      - name: config
        configMap:
          name: otel-collector-config</code></pre></p>
<p>#### Application Instrumentation Example
<pre><code>// Go application with OpenTelemetry tracing
package main</p>
<p>import (
    "context"
    "fmt"
    "log"
    "net/http"
    "time"</p>
<p>"go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/sdk/resource"
    "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
    "go.opentelemetry.io/otel/trace"
)</p>
<p>var tracer trace.Tracer</p>
<p>func initTracer() {
    ctx := context.Background()
    
    // Create OTLP exporter
    exporter, err := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint("otel-collector:4317"),
        otlptracegrpc.WithInsecure(),
    )
    if err != nil {
        log.Fatal("Failed to create exporter:", err)
    }
    
    // Create resource
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName("user-service"),
            semconv.ServiceVersion("1.2.3"),
            semconv.DeploymentEnvironment("production"),
        ),
    )
    if err != nil {
        log.Fatal("Failed to create resource:", err)
    }
    
    // Create trace provider
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exporter),
        trace.WithResource(res),
        trace.WithSampler(trace.AlwaysSample()),
    )
    
    otel.SetTracerProvider(tp)
    otel.SetTextMapPropagator(propagation.TraceContext{})
    
    tracer = otel.Tracer("user-service")
}</p>
<p>func userHandler(w http.ResponseWriter, r *http.Request) {
    // Extract trace context from incoming request
    ctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header))
    
    // Start new span
    ctx, span := tracer.Start(ctx, "handle_user_request",
        trace.WithAttributes(
            attribute.String("http.method", r.Method),
            attribute.String("http.url", r.URL.String()),
            attribute.String("user.id", r.URL.Query().Get("id")),
        ),
    )
    defer span.End()
    
    userID := r.URL.Query().Get("id")
    
    // Call database (creates child span)
    user, err := getUserFromDB(ctx, userID)
    if err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    // Call external service (creates child span)
    profile, err := getProfileFromService(ctx, userID)
    if err != nil {
        span.RecordError(err)
        // Don't fail the request for profile errors
        log.Printf("Failed to get profile: %v", err)
    }
    
    // Combine data and respond
    response := combineUserData(user, profile)
    
    span.SetAttributes(
        attribute.Bool("cache.hit", false),
        attribute.Int("response.size", len(response)),
    )
    
    w.Header().Set("Content-Type", "application/json")
    w.Write([]byte(response))
}</p>
<p>func getUserFromDB(ctx context.Context, userID string) (string, error) {
    ctx, span := tracer.Start(ctx, "database_query",
        trace.WithAttributes(
            attribute.String("db.system", "postgresql"),
            attribute.String("db.operation", "SELECT"),
            attribute.String("db.table", "users"),
            attribute.String("user.id", userID),
        ),
    )
    defer span.End()
    
    // Simulate database query
    time.Sleep(50 * time.Millisecond)
    
    if userID == "invalid" {
        err := fmt.Errorf("user not found")
        span.RecordError(err)
        return "", err
    }
    
    span.SetAttributes(attribute.Int("db.rows_affected", 1))
    return fmt.Sprintf(<code>{"id": "%s", "name": "John Doe"}</code>, userID), nil
}</p>
<p>func getProfileFromService(ctx context.Context, userID string) (string, error) {
    ctx, span := tracer.Start(ctx, "external_service_call",
        trace.WithAttributes(
            attribute.String("service.name", "profile-service"),
            attribute.String("http.method", "GET"),
            attribute.String("user.id", userID),
        ),
    )
    defer span.End()
    
    // Create HTTP request with trace context
    req, _ := http.NewRequestWithContext(ctx, "GET", 
        fmt.Sprintf("http://profile-service/users/%s", userID), nil)
    
    // Inject trace context into request headers
    otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))
    
    // Make request
    client := &http.Client{Timeout: 5 * time.Second}
    resp, err := client.Do(req)
    if err != nil {
        span.RecordError(err)
        return "", err
    }
    defer resp.Body.Close()
    
    span.SetAttributes(
        attribute.Int("http.status_code", resp.StatusCode),
        attribute.String("http.response.size", resp.Header.Get("Content-Length")),
    )
    
    if resp.StatusCode != 200 {
        err := fmt.Errorf("profile service returned %d", resp.StatusCode)
        span.RecordError(err)
        return "", err
    }
    
    return <code>{"preferences": {"theme": "dark"}}</code>, nil
}</p>
<p>func main() {
    initTracer()
    
    http.HandleFunc("/users", userHandler)
    log.Fatal(http.ListenAndServe(":8080", nil))
}</code></pre></p>
<p>#### Kubernetes Deployment with Tracing
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
      annotations:
        sidecar.opentelemetry.io/inject: "true"
    spec:
      containers:
      - name: user-service
        image: user-service:latest
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otel-collector:4318"
        - name: OTEL_SERVICE_NAME
          value: "user-service"
        - name: OTEL_SERVICE_VERSION
          value: "1.2.3"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "environment=production,team=backend"
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: 256Mi
            cpu: 200m
          limits:
            memory: 512Mi
            cpu: 500m</code></pre></p>
<h4>Jaeger Deployment</h4>
<p>#### Jaeger All-in-One (Development)
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.40
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14250
          name: grpc
        - containerPort: 14268
          name: http
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        resources:
          requests:
            memory: 512Mi
            cpu: 200m
          limits:
            memory: 1Gi
            cpu: 500m
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: observability
spec:
  selector:
    app: jaeger
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: grpc
    port: 14250
    targetPort: 14250
  - name: http
    port: 14268
    targetPort: 14268
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318</code></pre></p>
<h3>Key Concepts Summary</h3>
<li><strong>Logging Architecture</strong> - Container logs collected by runtime, aggregated by DaemonSets like Fluent Bit</li>
<li><strong>Metrics Collection</strong> - Prometheus pull-based model with exporters for different components</li>
<li><strong>Health Probes</strong> - Liveness (restart), readiness (traffic), and startup (slow start) checks</li>
<li><strong>Distributed Tracing</strong> - Request flow tracking through microservices with trace and span IDs</li>
<li><strong>OpenTelemetry</strong> - Unified observability framework for metrics, logs, and traces</li>
<li><strong>Grafana Dashboards</strong> - Visualization and alerting based on Prometheus metrics</li>
<li><strong>Jaeger</strong> - Distributed tracing backend for storing and analyzing traces</li>
<li><strong>Log Aggregation</strong> - Centralized logging with EFK/ELK stack for search and analysis</li>
<li><strong>Custom Metrics</strong> - Application-specific metrics exposed in Prometheus format</li>
<li><strong>Alert Management</strong> - Rule-based alerting with Prometheus and Alertmanager</li>
<h3>Best Practices / Tips</h3>
<p>1. <strong>Structure your logs</strong> - Use JSON logging for better parsing and filtering
2. <strong>Implement proper health checks</strong> - Different endpoints for liveness, readiness, and startup
3. <strong>Monitor the four golden signals</strong> - Latency, traffic, errors, and saturation
4. <strong>Use distributed tracing</strong> - Essential for debugging microservices interactions
5. <strong>Set up alerting</strong> - Proactive monitoring with appropriate alert thresholds
6. <strong>Monitor resource utilization</strong> - Track CPU, memory, disk, and network usage
7. <strong>Implement graceful shutdown</strong> - Proper handling of termination signals
8. <strong>Use sampling for traces</strong> - Avoid overwhelming trace storage with 100% sampling
9. <strong>Monitor business metrics</strong> - Not just technical metrics but business KPIs
10. <strong>Document your observability</strong> - Clear runbooks for common issues and metrics</p>
<h3>Common Issues / Troubleshooting</h3>
<h4>Problem 1: High Cardinality Metrics</h4>
<li><strong>Symptom:</strong> Prometheus consuming excessive memory and storage</li>
<li><strong>Cause:</strong> Metrics with too many unique label combinations</li>
<li><strong>Solution:</strong> Reduce label cardinality and use recording rules</li>
<pre><code><h2>Check high cardinality metrics</h2>
curl http://prometheus:9090/api/v1/label/__name__/values | jq '.data | length'
<h2>Find series with high cardinality</h2>
curl http://prometheus:9090/api/v1/query?query=count%20by%20(__name__)(%7B__name__%3D~%22.%2B%22%7D)</code></pre>
<h4>Problem 2: Health Probes Failing</h4>
<li><strong>Symptom:</strong> Pods restarting frequently or not receiving traffic</li>
<li><strong>Cause:</strong> Incorrectly configured probes or application issues</li>
<li><strong>Solution:</strong> Check probe configuration and application health endpoints</li>
<pre><code><h2>Check pod events for probe failures</h2>
kubectl describe pod pod-name
<h2>Test health endpoint manually</h2>
kubectl exec -it pod-name -- curl http://localhost:8080/health
<h2>Check probe configuration</h2>
kubectl get pod pod-name -o yaml | grep -A 10 -B 5 probe</code></pre>
<h4>Problem 3: Missing Traces</h4>
<li><strong>Symptom:</strong> Distributed traces not appearing in Jaeger</li>
<li><strong>Cause:</strong> Instrumentation issues or collector problems</li>
<li><strong>Solution:</strong> Verify OpenTelemetry configuration and collector status</li>
<pre><code><h2>Check OpenTelemetry collector logs</h2>
kubectl logs -l app=otel-collector -n observability
<h2>Verify trace export configuration</h2>
kubectl describe configmap otel-collector-config
<h2>Test direct trace submission</h2>
curl -X POST http://otel-collector:4318/v1/traces -H "Content-Type: application/json" -d '{...}'</code></pre>
<h4>Problem 4: Log Collection Issues</h4>
<li><strong>Symptom:</strong> Application logs not appearing in centralized logging</li>
<li><strong>Cause:</strong> Log collector configuration or permissions issues</li>
<li><strong>Solution:</strong> Check DaemonSet status and log paths</li>
<pre><code><h2>Check Fluent Bit DaemonSet status</h2>
kubectl get daemonset fluent-bit -n kube-system
<h2>Check Fluent Bit logs</h2>
kubectl logs -l app=fluent-bit -n kube-system
<h2>Verify log file permissions</h2>
kubectl exec -it fluent-bit-pod -- ls -la /var/log/containers/</code></pre>
<h4>Problem 5: Prometheus Scraping Failures</h4>
<li><strong>Symptom:</strong> Metrics not being collected from certain targets</li>
<li><strong>Cause:</strong> Service discovery issues or network connectivity problems</li>
<li><strong>Solution:</strong> Check Prometheus targets and service discovery</li>
<pre><code><h2>Check Prometheus targets</h2>
curl http://prometheus:9090/api/v1/targets
<h2>Check service discovery</h2>
curl http://prometheus:9090/api/v1/discovery
<h2>Verify pod annotations</h2>
kubectl get pods -o yaml | grep prometheus.io</code></pre>
<h3>References / Further Reading</h3>
<li>[Kubernetes Monitoring Architecture](https://kubernetes.io/docs/concepts/cluster-administration/monitoring/)</li>
<li>[Prometheus Documentation](https://prometheus.io/docs/)</li>
<li>[Grafana Documentation](https://grafana.com/docs/)</li>
<li>[OpenTelemetry Documentation](https://opentelemetry.io/docs/)</li>
<li>[Jaeger Documentation](https://www.jaegertracing.io/docs/)</li>
<li>[Fluent Bit Documentation](https://docs.fluentbit.io/)</li>
<li>[Kubernetes Health Checks](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)</li>
<li>[Elastic Stack Documentation](https://www.elastic.co/guide/index.html)</li>
<li>[Four Golden Signals](https://sre.google/sre-book/monitoring-distributed-systems/)</li></ul>
            </div>
        </div>
        
        <div class="note-footer">
            <p><a href="../index.html">← Back to Alex Susanu's Knowledge Base</a></p>
        </div>
    </div>
</body>
</html>