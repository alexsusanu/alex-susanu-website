<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CKA Guide: Kubernetes Cluster Components - Control Plane & Worker Node Architecture - Alex Susanu</title>
    <link rel="stylesheet" href="../assets/css/main.css">
    <style>
        /* Note-specific styles that extend the main CSS */
        .note-page {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .note-container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            min-height: 100vh;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
        }
        
        .note-header {
            background: linear-gradient(135deg, #4a90e2, #357abd);
            color: white;
            padding: 30px;
            text-align: center;
        }
        
        .back-nav {
            background: #f8f9ff;
            padding: 15px 30px;
            border-bottom: 2px solid #e8f0ff;
        }
        
        .back-btn {
            background: #4a90e2;
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            border-radius: 5px;
            font-size: 14px;
            transition: all 0.3s ease;
        }
        
        .back-btn:hover {
            background: #357abd;
        }
        
        .note-content-wrapper {
            padding: 40px 30px;
        }
        
        .note-meta {
            color: #666;
            font-style: italic;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 1px solid #e8f0ff;
        }
        
        .note-footer {
            background: #f8f9ff;
            padding: 20px 30px;
            text-align: center;
            color: #666;
            border-top: 2px solid #e8f0ff;
        }
    </style>
</head>
<body class="note-page">
    <div class="note-container">
        <div class="note-header">
            <h1>CKA Guide: Kubernetes Cluster Components - Control Plane & Worker Node Architecture</h1>
        </div>
        
        <div class="back-nav">
            <a href="../index.html" class="back-btn">← Back to Knowledge Base</a>
        </div>
        
        <div class="note-content-wrapper">
            <div class="note-meta">
                Kubernetes Certification (k8s) • Updated June 02, 2025
            </div>
            
            <div class="note-tags">
                <span class="tag">cka</span><span class="tag">kubernetes</span><span class="tag">exam</span><span class="tag">kubectl</span><span class="tag">certification</span>
            </div>
            
            <div class="note-content">
                <h2>CKA Guide: Kubernetes Cluster Components - Control Plane & Worker Node Architecture</h2>
<h3>Fundamental Conceptual Understanding</h3>
<h4>The Distributed Systems Architecture Philosophy</h4>
<strong>From Monolithic to Distributed Control Plane:</strong>
<pre><code>Traditional Infrastructure Management:
├── Single management server (SPOF)
├── Centralized state and decision making
├── Manual scaling and failover
├── Vendor-specific management interfaces
└── Limited automation capabilities
<p>Kubernetes Distributed Architecture:
├── Multiple independent, coordinating components
├── Shared state through distributed consensus (etcd)
├── Automatic leader election and failover
├── API-driven, vendor-neutral interfaces
└── Declarative automation at scale</code></pre></p>
<strong>The Microservices Pattern Applied to Infrastructure:</strong>
<pre><code>Each Kubernetes component follows microservices principles:
├── Single Responsibility: Each component has one primary function
├── API-First: All communication through well-defined APIs
├── Stateless Design: Components don't store state locally
├── Independent Deployment: Components can be updated separately
├── Fault Isolation: Component failure doesn't cascade
└── Horizontal Scaling: Multiple instances for high availability
<p>This design enables:
├── Operational flexibility and easier maintenance
├── Independent component scaling and optimization
├── Clear separation of concerns and debugging
├── Extensibility through plugin architectures
└── Vendor neutrality and cloud portability</code></pre></p>
<h4>Control Theory and Feedback Loops</h4>
<strong>The Control Plane as a Control System:</strong>
<pre><code>Kubernetes implements a classic control theory model:
<p>Input (Desired State) → Controller → System (Cluster) → Output (Actual State)
        ↑                                                        │
        └──────── Feedback Loop (Continuous Monitoring) ←────────┘</p>
<p>Components in the Control Loop:
├── API Server: Receives desired state declarations
├── Controllers: Implement control logic and take corrective actions
├── Scheduler: Optimizes resource allocation decisions
├── kubelet: Executes control decisions on worker nodes
└── etcd: Maintains authoritative record of desired and actual state</p>
<p>Feedback Mechanisms:
├── Continuous state observation through kubelet reports
├── Event-driven reactions to state changes
├── Reconciliation loops to correct drift
├── Health monitoring and automatic recovery
└── Metrics collection for performance optimization</code></pre></p>
<strong>State Convergence Philosophy:</strong>
<pre><code>Kubernetes operates on eventual consistency principles:
<p>1. Desired State Declaration:
   User submits YAML → API Server validates → etcd stores</p>
<p>2. State Observation:
   Controllers watch etcd → Compare desired vs actual → Identify drift</p>
<p>3. Corrective Action:
   Controllers take action → Update cluster state → Report back to etcd</p>
<p>4. Convergence:
   Repeat until actual state matches desired state</p>
<p>This model provides:
├── Self-healing capabilities (automatic error correction)
├── Declarative simplicity (describe what, not how)
├── Idempotent operations (safe to retry)
├── Distributed coordination (multiple controllers working together)
└── Resilient operations (continues working despite component failures)</code></pre></p>
<h3>Control Plane Components Deep Dive</h3>
<h4>kube-apiserver: The Central Nervous System</h4>
<strong>API Server Architecture and Responsibilities:</strong>
<pre><code>kube-apiserver Core Functions:
├── RESTful API Gateway: HTTP/gRPC interface for all cluster operations
├── Authentication & Authorization: Identity verification and permission checking
├── Admission Control: Request validation, mutation, and policy enforcement
├── Resource Validation: Schema validation and business logic enforcement
├── etcd Interface: Persistent storage backend communication
├── Watch API: Event streaming for real-time state monitoring
└── Aggregation Layer: Extension API integration point
<p>Request Processing Pipeline:
HTTP Request → Authentication → Authorization → Admission Controllers → 
Validation → Serialization → etcd Storage → Response</p>
<p>Watch Streaming:
etcd Change → API Server → Watch Stream → Controller → Action</code></pre></p>
<strong>API Server Configuration and Tuning:</strong>
<pre><code><h2>API Server configuration options (kubeadm example)</h2>
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    
    # Core API settings
    - --advertise-address=10.0.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,DefaultStorageClass,ResourceQuota
    
    # etcd connection
    - --etcd-servers=https://127.0.0.1:2379
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    
    # Service account and RBAC
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    
    # Network configuration
    - --service-cluster-ip-range=10.96.0.0/12
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    
    # Performance and reliability
    - --request-timeout=60s
    - --max-requests-inflight=400
    - --max-mutating-requests-inflight=200
    - --watch-cache-sizes=default=100
    
    # Security
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --audit-log-path=/var/log/audit.log
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
    
    image: k8s.gcr.io/kube-apiserver:v1.25.0
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 10.0.1.100
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 10.0.1.100
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15</code></pre>
<strong>API Server Health and Monitoring:</strong>
<pre><code><h2>Health check endpoints</h2>
curl -k https://localhost:6443/healthz
curl -k https://localhost:6443/livez
curl -k https://localhost:6443/readyz
<h2>Detailed health components</h2>
curl -k https://localhost:6443/livez?verbose=1
curl -k https://localhost:6443/readyz?verbose=1
<h2>API server metrics (if enabled)</h2>
curl -k https://localhost:6443/metrics
<h2>Common API server issues:</h2>
<h2>1. Certificate expiration</h2>
sudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep "Not After"
<h2>2. etcd connectivity</h2>
kubectl logs -n kube-system kube-apiserver-master | grep -i etcd
<h2>3. High request latency</h2>
kubectl logs -n kube-system kube-apiserver-master | grep -i "request took"
<h2>4. Authentication/authorization failures</h2>
kubectl logs -n kube-system kube-apiserver-master | grep -i "forbidden\|unauthorized"</code></pre>
<h4>etcd: The Distributed Database</h4>
<strong>etcd Architecture and Data Model:</strong>
<pre><code>etcd Design Principles:
├── Consistency: Strong consistency through Raft consensus algorithm
├── Availability: Highly available with leader election and failover
├── Partition Tolerance: Continues operating with network partitions
├── Simplicity: Simple key-value API with hierarchical keys
└── Performance: Optimized for read-heavy workloads with watch API
<p>Raft Consensus Algorithm:
├── Leader Election: One node becomes leader, others are followers
├── Log Replication: Leader replicates entries to followers
├── Commitment: Entries committed when majority acknowledges
├── Safety: Ensures consistency even during network partitions
└── Efficiency: Optimizes for normal case performance</p>
<p>etcd Data Organization:
├── Key-Value Store: Hierarchical keys like filesystem paths
├── Revisions: Every change gets monotonically increasing revision
├── Compaction: Old revisions removed to reclaim space
├── Transactions: Multi-key atomic operations
└── Watches: Real-time notification of key changes</code></pre></p>
<strong>etcd Operations and Maintenance:</strong>
<pre><code><h2>etcd cluster health check</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  endpoint health
<h2>Cluster member management</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  member list
<h2>Database size and performance monitoring</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  endpoint status --write-out=table
<h2>Backup creation</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  snapshot save /backup/etcd-snapshot-$(date +%Y%m%d%H%M%S).db
<h2>Database defragmentation (maintenance operation)</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  defrag
<h2>Compaction (remove old revisions)</h2>
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  compact $(etcdctl endpoint status --write-out="json" | jq '.[0].Status.header.revision')</code></pre>
<strong>etcd Performance Optimization:</strong>
<pre><code><h2>etcd configuration for performance</h2>
apiVersion: v1
kind: Pod
metadata:
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --name=master
    - --data-dir=/var/lib/etcd
    
    # Cluster configuration
    - --listen-client-urls=https://127.0.0.1:2379,https://10.0.1.100:2379
    - --advertise-client-urls=https://10.0.1.100:2379
    - --listen-peer-urls=https://10.0.1.100:2380
    - --initial-advertise-peer-urls=https://10.0.1.100:2380
    
    # Performance tuning
    - --heartbeat-interval=100        # Leader heartbeat interval (ms)
    - --election-timeout=1000         # Election timeout (ms)
    - --max-snapshots=5              # Maximum number of snapshots
    - --max-wals=5                   # Maximum number of WAL files
    - --quota-backend-bytes=8589934592  # 8GB backend size limit
    
    # Compaction settings
    - --auto-compaction-mode=periodic
    - --auto-compaction-retention=1h  # Retain 1 hour of history
    
    # Security
    - --client-cert-auth=true
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    
    image: k8s.gcr.io/etcd:3.5.4-0
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 1000m
        memory: 2Gi
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs</code></pre>
<h4>kube-scheduler: The Resource Optimizer</h4>
<strong>Scheduler Architecture and Algorithm:</strong>
<pre><code>Two-Phase Scheduling Process:
<p>Phase 1: Filtering (Feasible Nodes)
├── NodeResourcesFit: Node has sufficient CPU/memory
├── NodeAffinity: Node matches affinity expressions
├── PodTopologySpread: Ensures even distribution
├── Taints/Tolerations: Pod tolerates node taints
├── VolumeBinding: Required volumes can be provisioned
└── Custom Filters: Extension points for custom logic</p>
<p>Phase 2: Scoring (Optimal Selection)
├── NodeResourcesFit: Prefer nodes with more available resources
├── InterPodAffinity: Satisfy pod affinity/anti-affinity preferences
├── NodeAffinity: Prefer nodes matching affinity preferences
├── ImageLocality: Prefer nodes with required images already present
├── Custom Scorers: Extension points for custom optimization
└── Final Selection: Choose node with highest weighted score</p>
<p>Scheduling Framework:
├── QueueSort: Order pods by priority for scheduling
├── PreFilter: Early validation and setup for filtering
├── Filter: Eliminate nodes that cannot run the pod
├── PostFilter: Handle scheduling failures (preemption)
├── PreScore: Preparation for scoring phase
├── Score: Assign scores to feasible nodes
├── NormalizeScore: Normalize scores to 0-100 range
├── Reserve: Reserve resources on chosen node
├── Permit: Final approval before binding
├── PreBind: Pre-binding operations
├── Bind: Actually bind pod to node
└── PostBind: Post-binding cleanup and notification</code></pre></p>
<strong>Scheduler Configuration and Customization:</strong>
<pre><code><h2>Scheduler configuration</h2>
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration
profiles:
<ul><li>schedulerName: default-scheduler</li>
  plugins:
    # Enable/disable specific plugins
    score:
      enabled:
      - name: NodeResourcesFit
        weight: 100
      - name: InterPodAffinity
        weight: 50
      - name: NodeAffinity
        weight: 25
      disabled:
      - name: PodTopologySpread  # Disable if not needed
    
    filter:
      enabled:
      - name: NodeResourcesFit
      - name: NodeAffinity
      - name: PodTopologySpread
      
  pluginConfig:
  # Configure NodeResourcesFit scoring
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: LeastAllocated     # Prefer nodes with more free resources
        # Alternative: MostAllocated, RequestedToCapacityRatio
  
  # Configure InterPodAffinity
  - name: InterPodAffinity
    args:
      hardPodAffinityWeight: 100
      
  # Configure PodTopologySpread
  - name: PodTopologySpread
    args:
      defaultConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
<h2>Multiple scheduler profiles for different workload types</h2>
<li>schedulerName: high-performance-scheduler</li>
  plugins:
    score:
      enabled:
      - name: NodeResourcesFit
        weight: 100
      disabled:
      - name: InterPodAffinity  # Disable for performance
  pluginConfig:
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: MostAllocated     # Pack workloads tightly</code></pre>
<strong>Scheduler Debugging and Optimization:</strong>
<pre><code><h2>Scheduler health and status</h2>
kubectl get events | grep FailedScheduling
kubectl logs -n kube-system kube-scheduler-master
<h2>Debug scheduling decisions</h2>
kubectl get events --field-selector reason=FailedScheduling
kubectl describe pod <pending-pod-name>
<h2>Scheduler performance metrics</h2>
kubectl logs -n kube-system kube-scheduler-master | grep "Attempting to schedule pod"
kubectl logs -n kube-system kube-scheduler-master | grep "Successfully assigned"
<h2>Common scheduling issues:</h2>
<h2>1. Insufficient resources</h2>
kubectl describe nodes | grep -A 5 "Allocated resources"
<h2>2. Affinity/anti-affinity conflicts</h2>
kubectl get pod <pod-name> -o yaml | grep -A 20 affinity:
<h2>3. Taints preventing scheduling</h2>
kubectl describe nodes | grep -A 3 Taints:
<h2>4. Resource quotas blocking scheduling</h2>
kubectl describe quota --all-namespaces
<h2>5. Priority class issues</h2>
kubectl get priorityclasses
kubectl describe priorityclass high-priority</code></pre>
<h4>kube-controller-manager: The Automation Engine</h4>
<strong>Controller Manager Architecture:</strong>
<pre><code>Controller Manager Components:
<p>Core Controllers:
├── Node Controller: Manages node lifecycle and health
├── Replication Controller: Ensures desired replica count
├── Endpoints Controller: Populates service endpoints
├── Service Account Controller: Creates default service accounts
├── Token Controller: Manages service account tokens
├── Resource Quota Controller: Enforces resource limits
├── Namespace Controller: Manages namespace lifecycle
└── Persistent Volume Controller: Handles PV/PVC binding</p>
<p>Workload Controllers:
├── Deployment Controller: Manages deployment rollouts
├── ReplicaSet Controller: Ensures pod replica count
├── StatefulSet Controller: Manages stateful applications
├── DaemonSet Controller: Ensures pod runs on each node
├── Job Controller: Manages batch job execution
├── CronJob Controller: Schedules periodic jobs
└── HorizontalPodAutoscaler: Automatic pod scaling</p>
<p>Each Controller Implements:
├── Watch API: Monitor resource changes in etcd
├── Work Queue: Buffer and rate-limit operations
├── Reconciliation Loop: Compare desired vs actual state
├── Error Handling: Retry failed operations with backoff
├── Leader Election: Ensure only one active instance
└── Metrics: Report controller performance and health</code></pre></p>
<strong>Controller Manager Configuration:</strong>
<pre><code><h2>kube-controller-manager configuration</h2>
apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    
    # Core settings
    - --bind-address=127.0.0.1
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    
    # Controller settings
    - --controllers=*,bootstrapsigner,tokencleaner
    - --concurrent-deployment-syncs=5        # Deployment controller parallelism
    - --concurrent-replicaset-syncs=5        # ReplicaSet controller parallelism
    - --concurrent-service-syncs=1           # Service controller parallelism
    - --concurrent-namespace-syncs=10        # Namespace controller parallelism
    
    # Node controller settings
    - --node-monitor-period=5s               # How often to check node health
    - --node-monitor-grace-period=40s        # Grace period before marking unhealthy
    - --pod-eviction-timeout=5m0s           # Time to wait before evicting pods
    - --unhealthy-zone-threshold=0.55        # Threshold for unhealthy zone
    
    # Resource management
    - --kube-api-qps=20                      # API server request rate
    - --kube-api-burst=30                    # API server burst capacity
    
    # Leader election
    - --leader-elect=true
    - --leader-elect-lease-duration=15s
    - --leader-elect-renew-deadline=10s
    - --leader-elect-retry-period=2s
    
    # Service account management
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    
    image: k8s.gcr.io/kube-controller-manager:v1.25.0
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 200m
        memory: 512Mi</code></pre>
<strong>Controller Performance and Debugging:</strong>
<pre><code><h2>Controller manager health</h2>
curl -k https://localhost:10257/healthz
<h2>Controller metrics</h2>
curl -k https://localhost:10257/metrics
<h2>Check controller logs</h2>
kubectl logs -n kube-system kube-controller-manager-master
<h2>Monitor specific controller activity</h2>
kubectl logs -n kube-system kube-controller-manager-master | grep "deployment-controller"
kubectl logs -n kube-system kube-controller-manager-master | grep "replicaset-controller"
kubectl logs -n kube-system kube-controller-manager-master | grep "node-controller"
<h2>Common controller issues:</h2>
<h2>1. High API server load</h2>
kubectl logs -n kube-system kube-controller-manager-master | grep "rate limit"
<h2>2. Controller loop delays</h2>
kubectl logs -n kube-system kube-controller-manager-master | grep "slow"
<h2>3. Leader election issues</h2>
kubectl logs -n kube-system kube-controller-manager-master | grep "leader"
<h2>4. Resource conflicts</h2>
kubectl get events | grep "conflict"</code></pre>
<h3>Worker Node Components</h3>
<h4>kubelet: The Node Agent</h4>
<strong>kubelet Architecture and Responsibilities:</strong>
<pre><code>kubelet Core Functions:
├── Pod Lifecycle Management: Create, start, stop, and delete pods
├── Container Runtime Interface: Manage containers through CRI
├── Volume Management: Mount and unmount pod volumes
├── Network Setup: Configure pod networking through CNI
├── Health Monitoring: Execute liveness and readiness probes
├── Resource Management: Enforce resource limits and QoS
├── Node Status Reporting: Report node health to API server
├── Static Pod Management: Run control plane pods
├── Device Plugin Management: Expose specialized hardware
└── Certificate Management: Rotate kubelet certificates
<p>kubelet Watch Loops:
├── Pod Watch: Monitor assigned pods from API server
├── ConfigMap/Secret Watch: Update mounted configurations
├── Node Watch: React to node updates and taints
├── Volume Watch: Handle volume attachment/detachment
└── Image Watch: Preload required container images</p>
<p>Resource Management:
├── CPU CFS Quotas: Enforce CPU limits through cgroups
├── Memory Limits: OOM killer integration for memory limits
├── Disk Quotas: Ephemeral storage management
├── Device Allocation: GPU and specialized hardware assignment
└── QoS Classes: Priority-based resource allocation</code></pre></p>
<strong>kubelet Configuration:</strong>
<pre><code><h2>kubelet configuration file (/var/lib/kubelet/config.yaml)</h2>
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
<h2>Basic settings</h2>
clusterDNS:
<li>10.96.0.10</li>
clusterDomain: cluster.local
staticPodPath: /etc/kubernetes/manifests
<h2>Container runtime</h2>
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
<h2>Resource management</h2>
maxPods: 110                        # Maximum pods per node
podPidsLimit: 4096                  # Maximum PIDs per pod
enforceNodeAllocatable:
<li>pods</li>
<li>system-reserved</li>
<li>kube-reserved</li>
<h2>System reserved resources</h2>
systemReserved:
  cpu: 100m
  memory: 100Mi
  ephemeral-storage: 1Gi
<h2>Kubernetes reserved resources</h2>
kubeReserved:
  cpu: 100m
  memory: 100Mi
  ephemeral-storage: 1Gi
<h2>Eviction settings</h2>
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
<p>evictionSoft:
  memory.available: "300Mi"
  nodefs.available: "15%"</p>
<p>evictionSoftGracePeriod:
  memory.available: "1m30s"
  nodefs.available: "1m30s"</p>
<h2>Image management</h2>
imageMinimumGCAge: 2m
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
<h2>Logging</h2>
logging:
  format: json
  verbosity: 2
<h2>Container log management</h2>
containerLogMaxSize: 50Mi
containerLogMaxFiles: 5
<h2>Node status update frequency</h2>
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 5m
<h2>Health check settings</h2>
streamingConnectionIdleTimeout: 4h
syncFrequency: 1m
<h2>Feature gates</h2>
featureGates:
  RotateKubeletServerCertificate: true
  CSIDriverRegistry: true</code></pre>
<strong>kubelet Debugging and Monitoring:</strong>
<pre><code><h2>kubelet service status</h2>
sudo systemctl status kubelet
sudo journalctl -u kubelet -f
<h2>kubelet configuration</h2>
sudo cat /var/lib/kubelet/config.yaml
sudo cat /etc/kubernetes/kubelet.conf
<h2>kubelet health endpoints</h2>
curl -k https://localhost:10250/healthz
curl -k https://localhost:10250/metrics
<h2>Common kubelet issues:</h2>
<h2>1. Certificate expiration</h2>
sudo journalctl -u kubelet | grep certificate
sudo openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -text -noout
<h2>2. Container runtime issues</h2>
sudo crictl version
sudo crictl info
sudo systemctl status containerd
<h2>3. Network plugin issues</h2>
sudo journalctl -u kubelet | grep CNI
ls -la /etc/cni/net.d/
<h2>4. Resource pressure</h2>
kubectl describe node $(hostname) | grep -A 10 Conditions:
kubectl describe node $(hostname) | grep -A 15 "Allocated resources"
<h2>5. Pod eviction events</h2>
sudo journalctl -u kubelet | grep evict
kubectl get events | grep Evicted</code></pre>
<h4>Container Runtime: The Execution Engine</h4>
<strong>Container Runtime Interface (CRI) Architecture:</strong>
<pre><code>CRI Design:
├── Image Service: Pull, list, remove, and inspect images
├── Runtime Service: Create, start, stop, and remove containers
├── Streaming Service: Exec, attach, and port-forward operations
├── Stats Service: Container and image filesystem statistics
└── Event Service: Container lifecycle event notifications
<p>Runtime Implementations:
├── containerd: CNCF graduated project, Docker's successor
├── CRI-O: Lightweight CRI implementation for Kubernetes
├── Docker Engine: Legacy runtime with dockershim (deprecated)
├── gVisor: Sandboxed runtime for improved security
└── Kata Containers: VM-based containers for isolation</p>
<p>CRI Protocol Flow:
kubelet → gRPC → CRI Runtime → OCI Runtime → Container
   │        │         │            │           │
Request   API     containerd    runc/crun   Process</code></pre></p>
<strong>containerd Configuration:</strong>
<pre><code><h2>/etc/containerd/config.toml</h2>
version = 2
<p>[grpc]
  address = "/run/containerd/containerd.sock"
  max_recv_message_size = 16777216
  max_send_message_size = 16777216</p>
<p>[debug]
  level = "info"</p>
<p>[metrics]
  address = "127.0.0.1:1338"
  grpc_histogram = false</p>
<p>[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    enable_selinux = false
    enable_tls_streaming = false
    max_container_log_line_size = 16384
    disable_cgroup = false
    disable_apparmor = false
    restrict_oom_score_adj = false
    max_concurrent_downloads = 3
    disable_proc_mount = false
    unset_seccomp_profile = ""
    tolerate_missing_hugetlb_controller = true
    disable_hugetlb_controller = true
    ignore_image_defined_volumes = false
    
    [plugins."io.containerd.grpc.v1.cri".containerd]
      snapshotter = "overlayfs"
      default_runtime_name = "runc"
      no_pivot = false
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          runtime_engine = ""
          runtime_root = ""
          privileged_without_host_devices = false
          base_runtime_spec = ""
          
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true</p>
<p>[plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      max_conf_num = 1
      conf_template = ""
      
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
          endpoint = ["https://registry-1.docker.io"]</code></pre></p>
<strong>Container Runtime Operations:</strong>
<pre><code><h2>containerd operations</h2>
sudo crictl version
sudo crictl info
<h2>Container management</h2>
sudo crictl ps                     # List running containers
sudo crictl ps -a                  # List all containers
sudo crictl inspect <container-id> # Inspect container
sudo crictl logs <container-id>    # View container logs
sudo crictl exec -it <container-id> /bin/sh  # Execute in container
<h2>Image management</h2>
sudo crictl images                 # List images
sudo crictl pull nginx:latest      # Pull image
sudo crictl rmi <image-id>         # Remove image
sudo crictl imagefsinfo            # Image filesystem info
<h2>Pod management (crictl)</h2>
sudo crictl pods                   # List pod sandboxes
sudo crictl inspectp <pod-id>      # Inspect pod sandbox
sudo crictl stopp <pod-id>         # Stop pod sandbox
sudo crictl rmp <pod-id>           # Remove pod sandbox
<h2>Runtime debugging</h2>
sudo systemctl status containerd
sudo journalctl -u containerd -f
<h2>Performance monitoring</h2>
sudo crictl stats                  # Container resource usage
sudo crictl stats <container-id>   # Specific container stats</code></pre>
<h4>kube-proxy: The Network Proxy</h4>
<strong>kube-proxy Architecture and Modes:</strong>
<pre><code>kube-proxy Responsibilities:
├── Service Discovery: Implement ClusterIP virtual networking
├── Load Balancing: Distribute traffic across service endpoints
├── Network Address Translation: DNAT from service IP to pod IP
├── Health Checking: Route traffic only to healthy endpoints
└── External Access: Implement NodePort and LoadBalancer services
<p>Proxy Modes:</p>
<p>1. iptables Mode (Default):
   ├── Creates iptables rules for each service
   ├── DNAT traffic from service IP to pod IPs
   ├── Random selection among healthy endpoints
   ├── Lower resource usage
   └── Higher latency with many services</p>
<p>2. IPVS Mode (Advanced):
   ├── Uses Linux IPVS for in-kernel load balancing
   ├── Multiple load balancing algorithms
   ├── Better performance with many services
   ├── Requires IPVS kernel modules
   └── More sophisticated connection tracking</p>
<p>3. userspace Mode (Legacy):
   ├── User-space proxy process
   ├── Higher latency due to context switching
   ├── No longer recommended
   └── Maintained for compatibility</p>
<p>Load Balancing Algorithms (IPVS):
├── Round Robin (rr): Distribute requests sequentially
├── Least Connection (lc): Route to least connected endpoint
├── Destination Hashing (dh): Consistent routing based on destination
├── Source Hashing (sh): Session affinity based on source IP
├── Shortest Expected Delay (sed): Minimize expected delay
└── Never Queue (nq): No queuing, immediate processing</code></pre></p>
<strong>kube-proxy Configuration:</strong>
<pre><code><h2>kube-proxy configuration</h2>
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
<h2>Basic settings</h2>
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
<h2>Cluster configuration</h2>
clusterCIDR: 10.244.0.0/16
configSyncPeriod: 15m0s
<h2>Proxy mode selection</h2>
mode: "ipvs"                       # "iptables", "ipvs", or "userspace"
<h2>IPVS configuration</h2>
ipvs:
  scheduler: "rr"                  # Load balancing algorithm
  syncPeriod: 30s
  minSyncPeriod: 5s
  strictARP: true                  # Important for metallb compatibility
  tcpTimeout: 0s
  tcpFinTimeout: 0s
  udpTimeout: 0s
<h2>iptables configuration</h2>
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 1s
  syncPeriod: 30s
<h2>Connection tracking</h2>
conntrack:
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
<h2>Node port configuration</h2>
nodePortAddresses: []              # Bind to all interfaces
portRange: "30000-32767"
<h2>Health and metrics</h2>
healthzBindAddress: 0.0.0.0:10256
metricsBindAddress: 127.0.0.1:10249
<h2>Feature gates</h2>
featureGates:
  EndpointSliceProxying: true      # Use EndpointSlices instead of Endpoints</code></pre>
<strong>kube-proxy Debugging:</strong>
<pre><code><h2>kube-proxy status and logs</h2>
kubectl get pods -n kube-system | grep kube-proxy
kubectl logs -n kube-system <kube-proxy-pod>
<h2>Check proxy mode</h2>
kubectl logs -n kube-system <kube-proxy-pod> | grep "Using proxy mode"
<h2>iptables rules (iptables mode)</h2>
sudo iptables -t nat -L KUBE-SERVICES
sudo iptables -t nat -L KUBE-NODEPORTS
sudo iptables -t nat -L | grep <service-name>
<h2>IPVS rules (IPVS mode)</h2>
sudo ipvsadm -L -n
sudo ipvsadm -L -n -t <service-cluster-ip>:<port>
<h2>Service connectivity testing</h2>
kubectl run test-pod --image=busybox --rm -it -- wget -qO- http://<service-name>
kubectl run test-pod --image=busybox --rm -it -- nslookup <service-name>
<h2>Common kube-proxy issues:</h2>
<h2>1. Service endpoints not updating</h2>
kubectl get endpoints <service-name>
kubectl describe endpoints <service-name>
<h2>2. iptables rules not created</h2>
sudo iptables-save | grep <service-name>
<h2>3. IPVS modules not loaded</h2>
lsmod | grep ip_vs
sudo modprobe ip_vs
sudo modprobe ip_vs_rr
sudo modprobe ip_vs_wrr
sudo modprobe ip_vs_sh
<h2>4. Network policy conflicts</h2>
kubectl get networkpolicies
kubectl describe networkpolicy <policy-name></code></pre>
<h3>High Availability and Scaling Patterns</h3>
<h4>Control Plane High Availability</h4>
<strong>Multi-Master Architecture:</strong>
<pre><code>HA Control Plane Design:
<p>Load Balancer (External)
├── API Server 1 (Active)
├── API Server 2 (Active)
└── API Server 3 (Active)</p>
<p>etcd Cluster (Odd Number)
├── etcd Node 1 (Leader/Follower)
├── etcd Node 2 (Leader/Follower)
└── etcd Node 3 (Leader/Follower)</p>
<p>Controllers (Leader Election)
├── Controller Manager (Leader + Standby)
└── Scheduler (Leader + Standby)</p>
<p>Best Practices:
├── Odd number of etcd nodes (3, 5, 7)
├── Separate etcd from other components
├── Use external load balancer for API servers
├── Ensure network partitions don't split brain
└── Monitor leader election and failover</code></pre></p>
<strong>kubeadm HA Setup Pattern:</strong>
<pre><code><h2>Initialize first control plane node</h2>
sudo kubeadm init --control-plane-endpoint "cluster-endpoint:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16
<h2>Join additional control plane nodes</h2>
sudo kubeadm join cluster-endpoint:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash> \
  --control-plane \
  --certificate-key <certificate-key>
<h2>Join worker nodes</h2>
sudo kubeadm join cluster-endpoint:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash></code></pre>
<h4>Component Scaling and Performance</h4>
<strong>Resource Requirements by Component:</strong>
<pre><code>Component Resource Guidelines:
<p>API Server:
├── CPU: 1-2 cores + 0.1 cores per 1000 pods
├── Memory: 1-2GB + 50MB per 1000 pods
├── Disk: 50GB + growth for audit logs
└── Network: High bandwidth for client requests</p>
<p>etcd:
├── CPU: 2-4 cores for production
├── Memory: 2-8GB depending on cluster size
├── Disk: SSD required, 50GB+ with low latency
└── Network: Low latency between members</p>
<p>Controller Manager:
├── CPU: 0.5-1 cores + scaling with cluster size
├── Memory: 256MB-1GB depending on controllers
├── Disk: Minimal (stateless)
└── Network: Moderate API server communication</p>
<p>Scheduler:
├── CPU: 0.2-0.5 cores + scaling with pods
├── Memory: 128MB-512MB
├── Disk: Minimal (stateless)
└── Network: Moderate API server communication</p>
<p>kubelet:
├── CPU: 0.1-0.2 cores per node
├── Memory: 100-200MB per node
├── Disk: Container image storage
└── Network: Pod networking bandwidth</code></pre></p>
<h3>Exam Tips & Quick Reference</h3>
<h4>⚡ Essential Component Commands</h4>
<pre><code><h2>Cluster component health</h2>
kubectl cluster-info
kubectl get componentstatuses
<h2>Control plane pod logs</h2>
kubectl logs -n kube-system kube-apiserver-<master>
kubectl logs -n kube-system etcd-<master>
kubectl logs -n kube-system kube-scheduler-<master>
kubectl logs -n kube-system kube-controller-manager-<master>
<h2>Node component debugging</h2>
sudo systemctl status kubelet
sudo journalctl -u kubelet -f
sudo crictl ps
sudo crictl logs <container-id>
<h2>etcd operations</h2>
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  endpoint health</code></pre>
<h4>🎯 Common Exam Scenarios</h4>
<strong>Scenario 1: Debug Control Plane Component</strong>
<pre><code><h2>Check component status</h2>
kubectl get pods -n kube-system
kubectl describe pod <component-pod> -n kube-system
<h2>Check component logs</h2>
kubectl logs <component-pod> -n kube-system
<h2>Check configuration files</h2>
sudo cat /etc/kubernetes/manifests/<component>.yaml</code></pre>
<strong>Scenario 2: Worker Node Issues</strong>
<pre><code><h2>Check node status</h2>
kubectl get nodes
kubectl describe node <node-name>
<h2>Check kubelet</h2>
sudo systemctl status kubelet
sudo journalctl -u kubelet | tail -50
<h2>Check container runtime</h2>
sudo crictl version
sudo systemctl status containerd</code></pre>
<h4>🚨 Critical Gotchas</h4>
<p>1. <strong>etcd Backup</strong>: Always backup etcd before cluster operations
2. <strong>Certificate Expiration</strong>: Monitor certificate expiration dates
3. <strong>Resource Limits</strong>: Set appropriate resource limits for components
4. <strong>Network Connectivity</strong>: Ensure components can communicate
5. <strong>Leader Election</strong>: Only one active controller-manager/scheduler
6. <strong>Static Pod Path</strong>: Control plane pods in <code>/etc/kubernetes/manifests/</code>
7. <strong>Service Account Tokens</strong>: Required for component authentication</p>
<h3>WHY This Matters - The Deeper Philosophy</h3>
<h4>Distributed Systems Architecture Principles</h4>
<strong>The Microservices Architecture Applied to Infrastructure:</strong>
<pre><code>Traditional Monolithic Infrastructure:
├── Single management server
├── Centralized state and decision making
├── Single point of failure
├── Difficult to scale individual functions
└── Vendor lock-in and limited flexibility
<p>Kubernetes Distributed Architecture:
├── Multiple independent, coordinating components
├── Shared state through consensus
├── No single point of failure
├── Independent scaling and optimization
└── Vendor-neutral and extensible</p>
<p>This architectural choice enables:
├── Operational resilience through redundancy
├── Horizontal scaling of control plane functions
├── Independent component evolution and updates
├── Clear separation of concerns for debugging
└── Ecosystem extensibility through well-defined APIs</code></pre></p>
<strong>The Control Theory Model:</strong>
<pre><code>Kubernetes implements classical control system principles:
<p>Feedback Control Loop:
Desired State → Controller → System Action → Actual State → Feedback
     ↑                                                        │
     └────────── Error Correction (Reconciliation) ←─────────┘</p>
<p>This model provides:
├── Self-healing: Automatic correction of system drift
├── Declarative operation: Specify what, not how
├── Idempotent actions: Safe to retry operations
├── Eventual consistency: System converges to desired state
└── Distributed coordination: Multiple controllers working together</code></pre></p>
<h4>Production Engineering Implications</h4>
<strong>The Operational Excellence Model:</strong>
<pre><code>Component Isolation Benefits:
├── Fault isolation: Component failure doesn't cascade
├── Independent scaling: Scale components based on load
├── Rolling updates: Update components without full outage
├── Resource optimization: Tune resources per component
├── Debugging simplicity: Clear component boundaries
└── Team ownership: Different teams can own different components
<p>High Availability Design:
├── No single points of failure in any component
├── Leader election for stateful components
├── Consensus algorithms for consistent state
├── Health monitoring and automatic failover
└── Geographic distribution for disaster recovery</code></pre></p>
<strong>The Economic Model of Distributed Infrastructure:</strong>
<pre><code>Cost Structure:
├── Infrastructure costs: Hardware and cloud resources
├── Operational costs: Monitoring, maintenance, support
├── Reliability costs: Redundancy and disaster recovery
├── Performance costs: Over-provisioning for peak load
└── Innovation costs: Training and tool development
<p>ROI of Kubernetes Architecture:
├── Reduced operational overhead through automation
├── Improved resource utilization through efficient scheduling
├── Lower downtime costs through self-healing
├── Faster feature delivery through standardized platforms
└── Reduced vendor lock-in through portable abstractions</code></pre></p>
<h4>Career Development Implications</h4>
<strong>For the Exam:</strong>
<li><strong>Component Understanding</strong>: Know what each component does and how they interact</li>
<li><strong>Debugging Skills</strong>: Systematically troubleshoot component failures</li>
<li><strong>Configuration Knowledge</strong>: Understand key configuration parameters</li>
<li><strong>Architecture Thinking</strong>: Understand how components work together</li>
<strong>For Production Systems:</strong>
<li><strong>High Availability</strong>: Design and operate resilient control planes</li>
<li><strong>Performance Optimization</strong>: Tune components for scale and efficiency</li>
<li><strong>Security</strong>: Secure component communication and access</li>
<li><strong>Monitoring</strong>: Implement comprehensive observability</li>
<strong>For Your Career:</strong>
<li><strong>Systems Architecture</strong>: Understand distributed systems design patterns</li>
<li><strong>Platform Engineering</strong>: Build and operate infrastructure platforms</li>
<li><strong>Site Reliability</strong>: Ensure system reliability through proper design</li>
<li><strong>Technical Leadership</strong>: Guide teams in adopting cloud-native architectures</li></ul>
<p>Understanding Kubernetes components deeply teaches you how <strong>distributed systems actually work</strong> in practice. This knowledge is fundamental to the CKA exam and essential for anyone designing, operating, or debugging complex distributed infrastructure.</p>
<p>These components represent the state of the art in distributed systems engineering - understanding them gives you insight into how to build resilient, scalable systems that can handle real-world production demands while maintaining simplicity and operational excellence.</p>
            </div>
        </div>
        
        <div class="note-footer">
            <p><a href="../index.html">← Back to Alex Susanu's Knowledge Base</a></p>
        </div>
    </div>
</body>
</html>