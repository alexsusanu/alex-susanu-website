<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CKA Study Guide: Highly-Available Kubernetes Clusters - Alex Susanu</title>
    <link rel="stylesheet" href="../assets/css/main.css">
    <style>
        /* Note-specific styles that extend the main CSS */
        .note-page {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .note-container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            min-height: 100vh;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
        }
        
        .note-header {
            background: linear-gradient(135deg, #4a90e2, #357abd);
            color: white;
            padding: 30px;
            text-align: center;
        }
        
        .back-nav {
            background: #f8f9ff;
            padding: 15px 30px;
            border-bottom: 2px solid #e8f0ff;
        }
        
        .back-btn {
            background: #4a90e2;
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            border-radius: 5px;
            font-size: 14px;
            transition: all 0.3s ease;
        }
        
        .back-btn:hover {
            background: #357abd;
        }
        
        .note-content-wrapper {
            padding: 40px 30px;
        }
        
        .note-meta {
            color: #666;
            font-style: italic;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 1px solid #e8f0ff;
        }
        
        .note-footer {
            background: #f8f9ff;
            padding: 20px 30px;
            text-align: center;
            color: #666;
            border-top: 2px solid #e8f0ff;
        }
    </style>
</head>
<body class="note-page">
    <div class="note-container">
        <div class="note-header">
            <h1>CKA Study Guide: Highly-Available Kubernetes Clusters</h1>
        </div>
        
        <div class="back-nav">
            <a href="../index.html" class="back-btn">← Back to Knowledge Base</a>
        </div>
        
        <div class="note-content-wrapper">
            <div class="note-meta">
                Kubernetes Certification (k8s) • Updated June 02, 2025
            </div>
            
            <div class="note-tags">
                <span class="tag">cka</span><span class="tag">kubernetes</span><span class="tag">exam</span><span class="tag">kubectl</span><span class="tag">certification</span>
            </div>
            
            <div class="note-content">
                <h2>CKA Study Guide: Highly-Available Kubernetes Clusters</h2>
<h3><strong>The Fundamental Problem of Single Points of Failure</strong></h3>
<p>A basic kubeadm cluster has a critical weakness: if the control plane node fails, the entire cluster becomes unmanageable. While existing workloads continue running (pods keep serving traffic), you cannot:
<ul><li>Deploy new applications</li>
<li>Scale existing workloads  </li>
<li>Access cluster resources through kubectl</li>
<li>Respond to node failures or pod crashes</li>
<li>Manage secrets, configmaps, or any cluster state</li></p>
<p>This creates an unacceptable risk for production systems where downtime directly impacts revenue, user experience, and business operations.</p>
<h4>Why High Availability is More Complex Than Just "Add More Servers"</h4>
<p>Kubernetes control plane components are stateful and require careful coordination:
<li><strong>etcd</strong> maintains strict consistency requirements and needs quorum</li>
<li><strong>API server</strong> instances must all serve identical data from the same etcd</li>
<li><strong>Controller managers</strong> use leader election to prevent conflicting actions</li>
<li><strong>Schedulers</strong> coordinate to prevent double-scheduling pods</li>
<li><strong>Load balancing</strong> must intelligently route traffic to healthy API servers</li></p>
<p>This isn't just about redundancy - it's about maintaining cluster integrity while providing seamless failover.</p>
<p>---</p>
<h3><strong>Understanding High Availability Architecture Patterns</strong></h3>
<h4>The Control Plane HA Challenge</h4>
<p>In a single-node control plane, all components run on one machine:
<pre><code>┌─────────────────────────────────────┐
│           Control Plane             │
│  ┌─────────┐ ┌──────────────────┐   │
│  │  etcd   │ │   kube-apiserver │   │
│  └─────────┘ └──────────────────┘   │
│  ┌──────────────────────────────┐   │
│  │ kube-controller-manager      │   │
│  └──────────────────────────────┘   │
│  ┌──────────────────────────────┐   │
│  │ kube-scheduler               │   │
│  └──────────────────────────────┘   │
└─────────────────────────────────────┘</code></pre></p>
<p>For HA, we need multiple control plane nodes, but this creates coordination challenges:
<pre><code>┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│Control Plane │  │Control Plane │  │Control Plane │
│     Node 1   │  │     Node 2   │  │     Node 3   │
└──────────────┘  └──────────────┘  └──────────────┘
       │                 │                 │
       └─────────────────┼─────────────────┘
                         │
                   ┌──────────┐
                   │ etcd     │
                   │ Cluster  │
                   └──────────┘</code></pre></p>
<h4>The Two Main HA Topologies</h4>
<strong>Stacked etcd topology</strong> (etcd runs on control plane nodes):
<li>Pros: Simpler setup, fewer machines required</li>
<li>Cons: Coupled failure domains, more complex recovery</li>
<strong>External etcd topology</strong> (dedicated etcd cluster):
<li>Pros: Independent failure domains, easier etcd management</li>
<li>Cons: More infrastructure, additional complexity</li>
<p>---</p>
<h3><strong>Deep Dive: etcd Clustering and Quorum</strong></h3>
<h4>Why etcd Requires Odd Numbers</h4>
<p>etcd uses the Raft consensus algorithm, which requires a majority (quorum) to make decisions:</p>
<li><strong>1 node</strong>: Quorum = 1, can tolerate 0 failures</li>
<li><strong>3 nodes</strong>: Quorum = 2, can tolerate 1 failure  </li>
<li><strong>5 nodes</strong>: Quorum = 3, can tolerate 2 failures</li>
<li><strong>7 nodes</strong>: Quorum = 4, can tolerate 3 failures</li>
<strong>Why not even numbers?</strong> With 4 nodes, if they split 2-2 due to network partition, neither side has majority and the cluster becomes unavailable (split-brain).
<h4>Raft Consensus in Practice</h4>
<p>When a client writes to etcd:
1. <strong>Leader receives write</strong> request
2. <strong>Leader replicates</strong> to followers
3. <strong>Majority acknowledges</strong> the write
4. <strong>Leader commits</strong> and responds to client
5. <strong>Leader notifies followers</strong> to commit</p>
<p>This ensures consistency but requires majority availability:
<pre><code><h2>Check etcd cluster health</h2>
ETCDCTL_API=3 etcdctl endpoint health \
  --endpoints=https://10.0.1.10:2379,https://10.0.1.11:2379,https://10.0.1.12:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key</p>
<h2>Check cluster member status  </h2>
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://10.0.1.10:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key</code></pre>
<h4>etcd Performance Considerations</h4>
<strong>Disk I/O is critical</strong>: etcd commits every write to disk for durability. Use SSDs and monitor disk latency.
<strong>Network latency matters</strong>: Cross-region etcd clusters can become slow due to consensus round-trips.
<strong>Memory usage</strong>: etcd stores entire keyspace in memory. Monitor usage and set appropriate limits.
<pre><code><h2>Monitor etcd performance metrics</h2>
curl -L https://10.0.1.10:2381/metrics | grep etcd_disk_wal_fsync_duration
<h2>Check etcd database size</h2>
ETCDCTL_API=3 etcdctl endpoint status \
  --endpoints=https://10.0.1.10:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  --write-out=table</code></pre>
<p>---</p>
<h3><strong>Load Balancer Requirements and Patterns</strong></h3>
<h4>Why API Server Load Balancing is Complex</h4>
<p>Unlike typical web application load balancing, Kubernetes API servers require:
<li><strong>Persistent connections</strong>: kubectl watch operations need long-lived connections</li>
<li><strong>Health checking</strong>: Must detect API server failures quickly</li>
<li><strong>TLS termination handling</strong>: Can terminate TLS or pass-through</li>
<li><strong>Backup behavior</strong>: Must handle all backends being down gracefully</li></p>
<h4>Load Balancer Options</h4>
<strong>HAProxy Configuration Example</strong>:
<pre><code><h2>/etc/haproxy/haproxy.cfg</h2>
global
    log stdout local0
<p>defaults
    mode tcp
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms</p>
<p>frontend k8s-api
    bind *:6443
    default_backend k8s-api-backend</p>
<p>backend k8s-api-backend
    balance roundrobin
    option tcp-check
    server master1 10.0.1.10:6443 check fall 3 rise 2
    server master2 10.0.1.11:6443 check fall 3 rise 2  
    server master3 10.0.1.12:6443 check fall 3 rise 2</code></pre></p>
<pre><code><h2>Start HAProxy</h2>
systemctl enable --now haproxy
<h2>Verify load balancer is working</h2>
curl -k https://loadbalancer:6443/healthz</code></pre>
<strong>NGINX Configuration Example</strong>:
<pre><code><h2>/etc/nginx/nginx.conf</h2>
stream {
    upstream k8s-api {
        server 10.0.1.10:6443 max_fails=3 fail_timeout=30s;
        server 10.0.1.11:6443 max_fails=3 fail_timeout=30s;
        server 10.0.1.12:6443 max_fails=3 fail_timeout=30s;
    }
    
    server {
        listen 6443;
        proxy_pass k8s-api;
        proxy_timeout 10s;
        proxy_connect_timeout 1s;
    }
}</code></pre>
<strong>Cloud Load Balancer Considerations</strong>:
<li><strong>AWS NLB</strong>: Use TCP mode, enable cross-zone load balancing</li>
<li><strong>GCP Load Balancer</strong>: Configure proper health checks on /healthz</li>
<li><strong>Azure Load Balancer</strong>: Use Standard SKU for production</li>
<h4>Load Balancer Health Checking</h4>
<p>The API server provides health endpoints:
<pre><code><h2>Liveness probe (is the server running?)</h2>
curl -k https://api-server:6443/livez</p>
<h2>Readiness probe (is the server ready to serve traffic?)</h2>
curl -k https://api-server:6443/readyz
<h2>Overall health (combines multiple checks)</h2>
curl -k https://api-server:6443/healthz</code></pre>
<p>---</p>
<h3><strong>Setting Up Stacked etcd HA Cluster</strong></h3>
<h4>Prerequisites and Planning</h4>
<strong>Hardware Requirements</strong>:
<li>3 or 5 control plane nodes (odd number for quorum)</li>
<li>Each node: 4 CPUs, 8GB RAM minimum for production</li>
<li>Fast disks (SSDs) for etcd performance</li>
<li>Reliable network with low latency between nodes</li>
<strong>Network Planning</strong>:
<pre><code><h2>Example IP layout:</h2>
<h2>Load Balancer: 10.0.1.100:6443</h2>
<h2>Control Plane 1: 10.0.1.10</h2>
<h2>Control Plane 2: 10.0.1.11  </h2>
<h2>Control Plane 3: 10.0.1.12</h2>
<h2>Worker Nodes: 10.0.1.20-29</h2></code></pre>
<h4>Step 1: Initialize First Control Plane Node</h4>
<pre><code><h2>Create kubeadm configuration for HA</h2>
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.10
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
clusterName: production-cluster
controlPlaneEndpoint: "10.0.1.100:6443"  # Load balancer endpoint
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "10.244.0.0/16"
apiServer:
  extraArgs:
    audit-log-maxage: "30"
    audit-log-maxbackup: "10"
    audit-log-path: "/var/log/audit.log"
  extraVolumes:
  - name: audit-log
    hostPath: "/var/log"
    mountPath: "/var/log"
    readOnly: false
controllerManager:
  extraArgs:
    bind-address: "0.0.0.0"
scheduler:
  extraArgs:
    bind-address: "0.0.0.0"
etcd:
  local:
    dataDir: "/var/lib/etcd"
    extraArgs:
      listen-metrics-urls: "http://0.0.0.0:2381"
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: "0.0.0.0"
EOF
<h2>Initialize first control plane</h2>
kubeadm init --config=kubeadm-config.yaml --upload-certs</code></pre>
<strong>Key Parameters Explained</strong>:
<strong>controlPlaneEndpoint</strong>: This is crucial - it's the load balancer address that all nodes will use to communicate with the API server. Must be set during init and cannot be changed later.
<strong>--upload-certs</strong>: Uploads control plane certificates to a secret in the cluster, allowing other control plane nodes to download them automatically.
<h4>Step 2: Set Up kubectl and CNI</h4>
<pre><code><h2>Configure kubectl</h2>
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
<h2>Install CNI (Calico example)</h2>
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
<p>cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
EOF</code></pre></p>
<h4>Step 3: Join Additional Control Plane Nodes</h4>
<p>The <code>kubeadm init</code> output provides join commands for both control plane and worker nodes:</p>
<pre><code><h2>On additional control plane nodes (10.0.1.11, 10.0.1.12)</h2>
kubeadm join 10.0.1.100:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:1234567890abcdef... \
    --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7d42</code></pre>
<strong>What --certificate-key does</strong>: Downloads the uploaded certificates from the cluster secret, allowing this node to become a control plane without manual certificate copying.
<h4>Step 4: Verify HA Setup</h4>
<pre><code><h2>Check all control plane nodes are ready</h2>
kubectl get nodes -l node-role.kubernetes.io/control-plane
<h2>Verify etcd cluster health</h2>
kubectl -n kube-system exec etcd-master1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list
<h2>Test API server failover</h2>
kubectl get nodes
<h2>Stop one API server, verify cluster still works</h2>
systemctl stop kubelet  # on one control plane node
kubectl get nodes  # should still work via load balancer</code></pre>
<p>---</p>
<h3><strong>External etcd HA Cluster Setup</strong></h3>
<h4>Why Choose External etcd</h4>
<strong>Advantages</strong>:
<li><strong>Failure isolation</strong>: etcd failures don't affect API servers directly</li>
<li><strong>Independent scaling</strong>: Can optimize etcd hardware separately</li>
<li><strong>Easier backup/restore</strong>: Dedicated etcd management</li>
<li><strong>Security</strong>: Can apply different security policies to etcd</li>
<strong>Disadvantages</strong>:
<li><strong>More infrastructure</strong>: Additional servers required</li>
<li><strong>Network complexity</strong>: More network paths to secure and monitor</li>
<li><strong>Operational overhead</strong>: Two clusters to manage instead of one</li>
<h4>Setting Up External etcd Cluster</h4>
<strong>Step 1: Prepare etcd Nodes</strong>
<pre><code><h2>On each etcd node (etcd1, etcd2, etcd3)</h2>
<h2>Install etcd</h2>
ETCD_VER=v3.5.9
curl -L https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzf etcd-${ETCD_VER}-linux-amd64.tar.gz
mv etcd-${ETCD_VER}-linux-amd64/etcd* /usr/local/bin/</code></pre>
<strong>Step 2: Generate etcd Certificates</strong>
<pre><code><h2>Create CA for etcd</h2>
mkdir -p /etc/etcd/pki
cd /etc/etcd/pki
<h2>Generate CA</h2>
openssl genrsa -out ca-key.pem 2048
openssl req -new -x509 -key ca-key.pem -out ca.pem -days 3650 -subj "/CN=etcd-ca"
<h2>Generate server certificates for each etcd node</h2>
for node in etcd1 etcd2 etcd3; do
  openssl genrsa -out ${node}-key.pem 2048
  openssl req -new -key ${node}-key.pem -out ${node}.csr -subj "/CN=${node}" \
    -config <(echo "[req]"; echo "distinguished_name=req"; echo "[san]"; echo "subjectAltName=DNS:${node},DNS:localhost,IP:127.0.0.1,IP:${node_ip}")
  openssl x509 -req -in ${node}.csr -CA ca.pem -CAkey ca-key.pem -out ${node}.pem -days 365 -extensions san \
    -extfile <(echo "[san]"; echo "subjectAltName=DNS:${node},DNS:localhost,IP:127.0.0.1,IP:${node_ip}")
done</code></pre>
<strong>Step 3: Configure etcd Cluster</strong>
<pre><code><h2>/etc/systemd/system/etcd.service on etcd1 (10.0.1.20)</h2>
[Unit]
Description=etcd
Documentation=https://github.com/coreos
<p>[Service]
Type=notify
User=etcd
ExecStart=/usr/local/bin/etcd \
  --name=etcd1 \
  --data-dir=/var/lib/etcd \
  --listen-peer-urls=https://10.0.1.20:2380 \
  --listen-client-urls=https://10.0.1.20:2379,https://127.0.0.1:2379 \
  --advertise-client-urls=https://10.0.1.20:2379 \
  --initial-advertise-peer-urls=https://10.0.1.20:2380 \
  --initial-cluster=etcd1=https://10.0.1.20:2380,etcd2=https://10.0.1.21:2380,etcd3=https://10.0.1.22:2380 \
  --initial-cluster-token=etcd-cluster-1 \
  --initial-cluster-state=new \
  --cert-file=/etc/etcd/pki/etcd1.pem \
  --key-file=/etc/etcd/pki/etcd1-key.pem \
  --peer-cert-file=/etc/etcd/pki/etcd1.pem \
  --peer-key-file=/etc/etcd/pki/etcd1-key.pem \
  --trusted-ca-file=/etc/etcd/pki/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/pki/ca.pem
Restart=on-failure
RestartSec=5</p>
<p>[Install]
WantedBy=multi-user.target</code></pre></p>
<pre><code><h2>Start etcd on all nodes</h2>
systemctl enable --now etcd
<h2>Verify cluster</h2>
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://10.0.1.20:2379,https://10.0.1.21:2379,https://10.0.1.22:2379 \
  --cacert=/etc/etcd/pki/ca.pem</code></pre>
<strong>Step 4: Configure Kubernetes with External etcd</strong>
<pre><code><h2>kubeadm-external-etcd.yaml</h2>
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "10.0.1.100:6443"
etcd:
  external:
    endpoints:
    - https://10.0.1.20:2379
    - https://10.0.1.21:2379
    - https://10.0.1.22:2379
    caFile: /etc/kubernetes/pki/etcd/ca.pem
    certFile: /etc/kubernetes/pki/etcd/client.pem
    keyFile: /etc/kubernetes/pki/etcd/client-key.pem</code></pre>
<p>---</p>
<h3><strong>HA Components: Controller Manager and Scheduler</strong></h3>
<h4>Leader Election Mechanism</h4>
<p>Both kube-controller-manager and kube-scheduler use leader election to ensure only one instance is active at a time, preventing conflicting operations.</p>
<strong>How Leader Election Works</strong>:
1. Each instance tries to create/update a lease object in the API server
2. The instance that successfully updates the lease becomes the leader
3. Leader periodically renews the lease to maintain leadership
4. If leader fails to renew, other instances compete to become new leader
5. Non-leader instances remain in standby, ready to take over
<pre><code><h2>Check current leaders</h2>
kubectl get lease -n kube-system
<h2>View leader election logs</h2>
kubectl logs -n kube-system kube-controller-manager-master1 | grep leader
kubectl logs -n kube-system kube-scheduler-master1 | grep leader</code></pre>
<h4>Controller Manager HA Configuration</h4>
<p>In the static pod manifest <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code>:
<pre><code>spec:
  containers:
  - command:
    - kube-controller-manager
    - --bind-address=0.0.0.0  # Listen on all interfaces for metrics
    - --leader-elect=true      # Enable leader election
    - --leader-elect-lease-duration=15s
    - --leader-elect-renew-deadline=10s
    - --leader-elect-retry-period=2s
    # ... other flags</code></pre></p>
<strong>Key Parameters</strong>:
<li><strong>leader-elect-lease-duration</strong>: How long a lease is valid (15s default)</li>
<li><strong>leader-elect-renew-deadline</strong>: When leader must renew before losing leadership (10s default)  </li>
<li><strong>leader-elect-retry-period</strong>: How often non-leaders check for leadership opportunity (2s default)</li>
<h4>Scheduler HA Configuration</h4>
<p>Similar configuration in <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code>:
<pre><code>spec:
  containers:
  - command:
    - kube-scheduler
    - --bind-address=0.0.0.0
    - --leader-elect=true
    - --leader-elect-lease-duration=15s
    # ... other flags</code></pre></p>
<h4>Understanding Leader Election Timing</h4>
<strong>Failover Time</strong>: If a leader dies, failover takes approximately:
<li>Leader detection: lease-duration (15s)</li>
<li>New leader election: retry-period * attempts (2-6s)</li>
<li><strong>Total</strong>: ~20s maximum downtime</li>
<strong>Tuning Considerations</strong>:
<li><strong>Shorter timers</strong>: Faster failover, more API server load</li>
<li><strong>Longer timers</strong>: Slower failover, less API server load</li>
<li><strong>Production recommendation</strong>: Use defaults unless you have specific requirements</li>
<p>---</p>
<h3><strong>Monitoring and Alerting for HA Clusters</strong></h3>
<h4>Critical Metrics to Monitor</h4>
<strong>etcd Health Metrics</strong>:
<pre><code><h2>Key metrics to watch:</h2>
<h2>- etcd_server_is_leader: Which node is leader (should be 1)</h2>
<h2>- etcd_server_leader_changes_seen_total: Leader changes (should be rare)</h2>
<h2>- etcd_disk_wal_fsync_duration_seconds: Disk performance</h2>
<h2>- etcd_network_peer_round_trip_time_seconds: Network latency</h2>
<h2>Scrape etcd metrics</h2>
curl -L https://10.0.1.20:2381/metrics | grep etcd_server_is_leader</code></pre>
<strong>API Server Health</strong>:
<pre><code><h2>Health endpoints for monitoring</h2>
curl -k https://10.0.1.10:6443/livez
curl -k https://10.0.1.10:6443/readyz
<h2>Metrics endpoint</h2>
curl -k https://10.0.1.10:6443/metrics | grep apiserver_request_duration</code></pre>
<strong>Controller Manager and Scheduler</strong>:
<pre><code><h2>Check leader status</h2>
kubectl get endpoints -n kube-system kube-controller-manager -o yaml
kubectl get endpoints -n kube-system kube-scheduler -o yaml
<h2>Metrics (if enabled)</h2>
curl http://10.0.1.10:10257/metrics  # controller-manager
curl http://10.0.1.10:10259/metrics  # scheduler</code></pre>
<h4>Essential Alerts</h4>
<strong>etcd Alerts</strong>:
<li>etcd cluster has no leader for > 1 minute</li>
<li>etcd leader changes > 3 times per hour</li>
<li>etcd disk fsync duration > 100ms</li>
<li>etcd cluster size != expected size</li>
<strong>API Server Alerts</strong>:
<li>API server down on > 1 control plane node</li>
<li>API server response time > 1s for 95th percentile</li>
<li>API server error rate > 5%</li>
<strong>Control Plane Alerts</strong>:
<li>Controller manager leader absent for > 30s</li>
<li>Scheduler leader absent for > 30s</li>
<li>Control plane node down for > 5 minutes</li>
<h4>Prometheus Monitoring Setup</h4>
<pre><code><h2>ServiceMonitor for etcd</h2>
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: etcd
  endpoints:
  - port: http-metrics
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
      certFile: /etc/prometheus/secrets/etcd-certs/client.crt
      keyFile: /etc/prometheus/secrets/etcd-certs/client.key</code></pre>
<p>---</p>
<h3><strong>Troubleshooting HA Cluster Issues</strong></h3>
<h4>Common etcd Problems</h4>
<strong>Split Brain Detection</strong>:
<pre><code><h2>Check if etcd cluster has quorum</h2>
ETCDCTL_API=3 etcdctl endpoint status \
  --endpoints=https://10.0.1.20:2379,https://10.0.1.21:2379,https://10.0.1.22:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  --write-out=table
<h2>Look for:</h2>
<h2>- Is Leader column (should have exactly one "true")  </h2>
<h2>- DB Size differences (should be similar)</h2>
<h2>- Raft Term (should be same across healthy members)</h2></code></pre>
<strong>etcd Performance Issues</strong>:
<pre><code><h2>Check disk performance</h2>
iostat -x 1 5  # Watch disk utilization
<h2>Check etcd metrics</h2>
curl -L https://10.0.1.20:2381/metrics | grep -E "(fsync_duration|backend_commit_duration)"
<h2>Monitor etcd logs</h2>
journalctl -u etcd -f | grep -E "(slow|timeout|failed)"</code></pre>
<strong>etcd Member Recovery</strong>:
<pre><code><h2>Remove failed member</h2>
ETCDCTL_API=3 etcdctl member remove MEMBER_ID \
  --endpoints=https://10.0.1.20:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt
<h2>Add new member</h2>
ETCDCTL_API=3 etcdctl member add etcd3 \
  --peer-urls=https://10.0.1.22:2380 \
  --endpoints=https://10.0.1.20:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt</code></pre>
<h4>API Server Load Balancer Issues</h4>
<strong>Health Check Failures</strong>:
<pre><code><h2>Test load balancer endpoints directly</h2>
curl -k https://10.0.1.10:6443/healthz
curl -k https://10.0.1.11:6443/healthz
curl -k https://10.0.1.12:6443/healthz
<h2>Check HAProxy stats (if using HAProxy)</h2>
echo "show stat" | socat stdio /var/lib/haproxy/stats
<h2>Test through load balancer</h2>
curl -k https://10.0.1.100:6443/healthz</code></pre>
<strong>Certificate Issues</strong>:
<pre><code><h2>Check certificate validity on each API server</h2>
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A2 "Not After"
<h2>Test TLS connection</h2>
openssl s_client -connect 10.0.1.10:6443 -servername kubernetes</code></pre>
<h4>Controller Manager and Scheduler Issues</h4>
<strong>Leader Election Problems</strong>:
<pre><code><h2>Check which instance is leader</h2>
kubectl get endpoints -n kube-system kube-controller-manager -o yaml
kubectl get endpoints -n kube-system kube-scheduler -o yaml
<h2>Look for frequent leader changes</h2>
kubectl logs -n kube-system kube-controller-manager-master1 | grep "became leader"
kubectl logs -n kube-system kube-controller-manager-master1 | grep "lost leader"
<h2>Check for connectivity issues between nodes</h2>
ping 10.0.1.11  # from each control plane node</code></pre>
<strong>Performance Issues</strong>:
<pre><code><h2>Check controller manager queue depth</h2>
kubectl logs -n kube-system kube-controller-manager-master1 | grep "queue depth"
<h2>Monitor resource usage</h2>
kubectl top pods -n kube-system | grep -E "(controller-manager|scheduler)"</code></pre>
<p>---</p>
<h3><strong>Disaster Recovery for HA Clusters</strong></h3>
<h4>Backup Strategy</h4>
<strong>Regular etcd Backups</strong>:
<pre><code>#!/bin/bash
<h2>backup-etcd.sh</h2>
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/etcd"
<p>ETCDCTL_API=3 etcdctl snapshot save ${BACKUP_DIR}/etcd-backup-${DATE}.db \
  --endpoints=https://10.0.1.20:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key</p>
<h2>Verify backup</h2>
ETCDCTL_API=3 etcdctl snapshot status ${BACKUP_DIR}/etcd-backup-${DATE}.db
<h2>Cleanup old backups (keep last 7 days)</h2>
find ${BACKUP_DIR} -name "etcd-backup-*.db" -mtime +7 -delete</code></pre>
<strong>Certificate and Configuration Backup</strong>:
<pre><code><h2>Backup control plane configurations</h2>
tar -czf control-plane-backup-$(date +%Y%m%d).tar.gz \
  /etc/kubernetes/pki/ \
  /etc/kubernetes/*.conf \
  /etc/kubernetes/manifests/</code></pre>
<h4>Cluster Recovery Scenarios</h4>
<strong>Total Cluster Loss with etcd Backup</strong>:
<pre><code><h2>1. Restore etcd from backup</h2>
ETCDCTL_API=3 etcdctl snapshot restore etcd-backup.db \
  --data-dir=/var/lib/etcd-restored
<h2>2. Initialize first control plane with restored data</h2>
kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd
<h2>3. Replace etcd data</h2>
systemctl stop kubelet
rm -rf /var/lib/etcd/*
mv /var/lib/etcd-restored/* /var/lib/etcd/
systemctl start kubelet
<h2>4. Rejoin other control plane nodes</h2>
kubeadm join --control-plane ...</code></pre>
<strong>Single Control Plane Node Recovery</strong>:
<pre><code><h2>If control plane node fails but etcd data is intact:</h2>
<h2>1. Reinstall OS and Kubernetes components</h2>
<h2>2. Copy certificates from other control plane nodes</h2>
scp -r master1:/etc/kubernetes/pki/ /etc/kubernetes/
<h2>3. Rejoin as control plane</h2>
kubeadm join --control-plane --certificate-key <key></code></pre>
<strong>etcd Member Recovery</strong>:
<pre><code><h2>If single etcd member fails:</h2>
<h2>1. Remove failed member from cluster</h2>
ETCDCTL_API=3 etcdctl member remove <member-id>
<h2>2. Clean data directory on failed node</h2>
rm -rf /var/lib/etcd/*
<h2>3. Re-add member and start etcd</h2>
ETCDCTL_API=3 etcdctl member add etcd3 --peer-urls=https://10.0.1.22:2380
<h2>Start etcd with --initial-cluster-state=existing</h2></code></pre>
<p>---</p>
<h3><strong>Best Practices for Production HA Clusters</strong></h3>
<h4>Infrastructure Design</h4>
<strong>Geographic Distribution</strong>:
<li>Spread control plane nodes across availability zones</li>
<li>Consider network latency impact on etcd performance</li>
<li>Use dedicated control plane nodes (no workloads)</li>
<strong>Hardware Specifications</strong>:
<li>Control plane: 4+ CPUs, 8+ GB RAM, SSD storage</li>
<li>etcd (if external): 8+ CPUs, 16+ GB RAM, high-IOPS SSD</li>
<li>Network: Low latency, high bandwidth between control plane nodes</li>
<strong>Security Hardening</strong>:
<pre><code><h2>Restrict etcd access</h2>
iptables -A INPUT -p tcp --dport 2379 -s 10.0.1.0/24 -j ACCEPT
iptables -A INPUT -p tcp --dport 2379 -j DROP
<h2>Use TLS for all components</h2>
<h2>Rotate certificates regularly</h2>
kubeadm certs renew all
<h2>Enable audit logging</h2>
<h2>Set resource limits for control plane pods</h2></code></pre>
<h4>Operational Procedures</h4>
<strong>Change Management</strong>:
<li>Test all changes in staging HA cluster first</li>
<li>Update one control plane node at a time</li>
<li>Verify cluster health after each change</li>
<li>Maintain rollback procedures</li>
<strong>Monitoring and Alerting</strong>:
<li>Monitor etcd performance and cluster health</li>
<li>Alert on leader election changes</li>
<li>Track API server response times and error rates</li>
<li>Monitor certificate expiration dates</li>
<strong>Regular Maintenance</strong>:
<pre><code><h2>Weekly etcd defragmentation</h2>
ETCDCTL_API=3 etcdctl defrag \
  --endpoints=https://10.0.1.20:2379,https://10.0.1.21:2379,https://10.0.1.22:2379
<h2>Monthly certificate rotation testing</h2>
kubeadm certs check-expiration
<h2>Quarterly disaster recovery testing</h2>
<h2>Practice full cluster recovery from backups</h2></code></pre>
<p>---</p>
<h3><strong>Exam Tips</strong></h3>
<h4>Key Concepts to Master</h4>
<li><strong>etcd quorum requirements</strong>: Understand odd numbers and failure tolerance</li>
<li><strong>Load balancer configuration</strong>: Know how to set up and troubleshoot</li>
<li><strong>Leader election</strong>: Understand how controller-manager and scheduler coordinate</li>
<li><strong>Certificate management</strong>: Know certificate locations and renewal</li>
<h4>Common Scenarios</h4>
1. <strong>Set up 3-node HA cluster with stacked etcd</strong>
2. <strong>Configure external load balancer for API servers</strong> 
3. <strong>Troubleshoot etcd cluster health issues</strong>
4. <strong>Recover from control plane node failure</strong>
5. <strong>Verify HA components are working correctly</strong>
<h4>Time-Saving Commands</h4>
<pre><code><h2>Quick HA cluster validation</h2>
kubectl get nodes -l node-role.kubernetes.io/control-plane
kubectl get pods -n kube-system -l component=etcd
kubectl get endpoints -n kube-system kube-controller-manager
<h2>Fast troubleshooting</h2>
kubectl cluster-info
journalctl -u kubelet -f
ETCDCTL_API=3 etcdctl member list --write-out=table</code></pre>
<h4>Critical Details</h4>
<li>controlPlaneEndpoint must be set during kubeadm init (cannot change later)</li>
<li>--upload-certs is required for additional control plane nodes to join</li>
<li>etcd backup location: <code>/var/lib/etcd</code> (default)</li>
<li>Leader election happens at endpoint level, not configmap level</li>
<li>Load balancer must health check /healthz, not just port connectivity</li></ul>
            </div>
        </div>
        
        <div class="note-footer">
            <p><a href="../index.html">← Back to Alex Susanu's Knowledge Base</a></p>
        </div>
    </div>
</body>
</html>